[
    {
        "symbol": "ALAB",
        "quarter": 3,
        "year": 2024,
        "date": "2024-11-04 19:32:07",
        "content": "Operator: Hello. Good afternoon. My name is Jeremy, and I will be your conference operator today. At this time, I would like to welcome everyone to the Astera Labs Q3 2024 Earnings Conference Call. All lines have been placed on mute to prevent any background noise. After management's remarks, there will be a question-and-answer session. [Operator Instructions]. Thank you. I will now turn the call over to Leslie Green, Investor Relations for Astera Labs. Leslie, you may begin.\nLeslie Green: Thank you, Jeremy, and good afternoon, everyone, and welcome to the Astera Labs third quarter 2024 earnings conference call. Joining us on the call today are Jitendra Mohan, Chief Executive Officer and Co-Founder; Sanjay Gajendra, President and Chief Operating Officer and Co-Founder; and Mike Tate, Chief Financial Officer. Before we get started, I would like to remind everyone that certain comments made in this call today may include forward-looking statements regarding, among other things, expected future financial results, strategies and plans, future operations and the markets in which we operate. These forward-looking statements reflect management's current beliefs, expectations and assumptions about future events which are inherently subject to risks and uncertainties that are discussed in today's earnings release and the periodic reports we file from time to time with the SEC, including risks set forth in the final prospectus relating to our IPO. It is not possible for the company's management to predict all risks and uncertainties that could have an impact on these forward-looking statements or the extent to which any factor or combination of factors may cause actual results to differ materially from those contained in any forward-looking statement. In light of these risks and uncertainties and assumptions, the results, events or circumstances reflected in the forward-looking statements discussed during this call may not occur, and actual results could differ materially from those anticipated or implied. All of our statements are made based on information available to management as of today, and the company undertakes no obligation to update such statements after the date of this call to conform to these as a result of new information, future events or changes in our expectations, except as required by law. Also during this call, we will refer to certain non-GAAP financial measures, which we consider to be an important measure of the company's performance. These non-GAAP financial measures are provided in addition to and not as a substitute for or superior to financial results prepared in accordance with U.S. GAAP. A discussion of why we use non-GAAP financial measures and reconciliations between our GAAP and non-GAAP financial measures is available in the earnings release we issued today, which can be accessed through the Investor Relations portion of our website and will also be included in our filings with the SEC, which will also be accessible through the Investor Relations portion of our website. With that, I would like to turn the call over to Jitendra Mohan, CEO of Astera Labs.\nJitendra Mohan: Thank you, Leslie. Good afternoon, everyone, and thanks for joining our third quarter conference call for fiscal year 2024. Today, I will provide a quick Q3 financial overview, discuss several of our recent company-specific product and strategic announcements, followed by a discussion around the key secular trends driving our market opportunity. I will then turn the call over to Sanjay to delve deeper into our growth strategy. Finally, Mike will provide additional details on our Q3 results and our Q4 financial guidance. Astera Labs delivered strong Q3 results, setting our fifth consecutive record for quarterly revenue at $113 million, which was up 47% from the last quarter and up 206% versus the prior year. Our business has entered a new growth phase with multiple product families ramping across AI platforms, featuring both third-party GPUs and internally developed AI accelerators, which drove the Q3 sales upside versus our guidance. We also demonstrated strong operating leverage during Q3, with non-GAAP operating margin expanding to over 32% and delivered non-GAAP EPS of $0.23. Looking into Q4, we expect our revenue momentum to continue, largely driven by the Aries PCIe and Taurus Ethernet product lines. The Scorpio Fabric Switches is continuing to ship in preproduction volumes. The criticality of connectivity in modern AI clusters continues to grow with trillion parameter model sizes, multistep reasoning models and faster, more complex AI accelerators. These developments present a tremendous opportunity for Astera Labs' intelligent connectivity platform to enhance AI server performance and productivity with our differentiated hardware and software solutions. At the start of Q4, we announced our fourth product line, the Scorpio Smart Fabric Switch family, which expands our mission of solving the increasingly complex connectivity challenges within AI infrastructure, both for scale out and for scale up networks. Extending beyond our current market footprint of PCI Express and Ethernet Retimer class products and controller class devices for CXL memory, our Scorpio Smart Fabric Switch family delivers meaningfully higher functionality and value to our AI and cloud infrastructure customers. We estimate that Scorpio will expand our total market opportunity for our four product families to more than $12 billion by 2028. Scorpio family unlocks a large and growing opportunity across AI Server Head Node scale-out applications with the P-Series and AI accelerators scale up clustering use cases with the X-Series. The P-Series devices directly address the challenge of keeping modern GPUs fed with data at ever-increasing speeds while the X-Series devices improve the efficiency and size of AI clusters. The Scorpio family was purpose-built from the ground up for these AI-specific workloads with close alignment with our hyperscaler and AI platform partners. At the recent 2024 OCP Global Summit, we demonstrated the industry's first PCIe Gen 6 fabric switch, which is currently shipping in preproduction volumes for AI platforms. We are happy to report that we already have design wins of both Scorpio P-Series and X-Series and that our recent product launches further accelerated strong customer and ecosystem interest. Over the coming quarters, we expect to further expand our business opportunities for the Scorpio product family across PCIe Gen 5, PCIe Gen 6 and platform-specific customized connectivity platforms. Further expanding our market opportunity, Astera Labs has joined the Ultra Accelerator Link, UALink Consortium as a promoting member on the Board of Directors, along with industry-leading hyperscalers and AI platform providers. This important industry initiative places us at the forefront of developing and advancing an open high-speed, low-latency interconnect for scale-up connectivity between accelerators. Astera Labs' deep expertise in developing advanced silicon-based connectivity solutions, along with a strong track record of technology execution makes us uniquely suited to contribute to this compelling and necessary industry initiative. We will shift towards shorter AI platform refresh cycles. Hyperscalers are increasingly relying on their trusted partners as they deploy new hardware in their data center infrastructure at an unprecedented pace and scale. To date, we have demonstrated strong execution with our Aries, Taurus, Leo and now Scorpio product family. Our products increase data, networking, memory bandwidth and capacity and our COSMOS software provides our hyperscaler customers with the tools necessary to monitor and observe the health of their expensive infrastructure to ensure maximum system utilization. To conclude, Astera Labs expanding product portfolio, including the new Scorpio Smart Fabric Switches, is cementing our position as a critical part of AI connectivity infrastructure, delivering increased value to our hyperscaler customers and unlocking additional multiyear growth trajectories for Astera Labs. With that, let me turn the call over to our President and COO, Sanjay Gajendra, to discuss some of our recent product announcements and our long-term growth strategy.\nSanjay Gajendra: Thanks, Jitendra, and good afternoon, everyone. We are pleased with our Q3 results and strong financial outlook for Q4. Overall, we believe we have entered a new phase at the company based on two key factors; first is the increased diversity of our business. In 3Q, our business diversified significantly with new product lines and form factors going to high-volume production. We also started ramping on multiple new AI platforms based on internally developed AI accelerators at multiple customers to go along with the continued momentum with third party GPU-based AI platforms. These together helped achieve a record sequential growth in 3Q. Second, with the introduction of Scorpio Smart Fabric Switches, our market opportunity has significantly expanded. This new product line delivers increased value to our hyperscaler customers and for Astera Labs unlocks higher dollar content in AI platforms, an additional multiyear growth trajectories. Let me explain our business drivers in more detail. As noted, we now have multiple product lines generations and form factors in high-volume production. This includes Aries Smart DSP Retimers and smart cable modules for PCIe 5.0 and Taurus smart cable modules for 200 and 400-gig Ethernet active electrical cables. We are also shipping preproduction volumes for Leo CXL memory controller\u2019s times for PCIe 6.0 and Scorpio fabric switch solutions for PCIe head node connectivity. All these new products deliver increased value to our customers and therefore, command higher ASPs. Our first market Scorpio PCIe Gen 6 fabric switch addresses a multibillion-dollar opportunity with a ground-up architecture designed for AI data flows and delivers maximum predictable performance per watt in mixed-mode PCIe head note connectivity compared to incumbent solutions. We are currently shipping Scorpio P-Series in preproduction quantities to support qualification for customized AI platforms based upon leading third-party GPUs. Interest for Scorpio P-Series has accelerated since the former launch given its differentiated features and as a result, we are engaging in multiple design opportunities across a diverse spectrum of AI platforms. These include both PCIe Gen 6 and PCIe Gen 5 implementations on third-party GPU and internal accelerator-based platforms. Overall, we are very bullish on the market for our entire product portfolio across third-party GPU-based systems with increasing dollar content per GPU on our new design wins and we believe that Astera's opportunity with internally developed AI accelerator platforms can be even larger with opportunities both in the scale-out and back-end scale-up clustering use cases. This additional market opportunity has unlocked design activity for Aries and Taurus product lines for reach extension within and between racks and for our newly introduced Scorpio X-Series Fabric Switches for homogeneous accelerator to accelerator connectivity maximum bandwidth. The Scorpio X-Series is built upon our software-defined architecture and leverages our cost more software suite to support a variety of platform-specific customization which provide hyperscalers with valuable flexibility. As we look ahead to 2025, we will begin to ramp designs across new internally developed AI accelerator-based platforms that will incorporate multiple Astera Labs product families, including Aries, Taurus and Scorpio. As a result, we will continue to benefit from increased dollar content per accelerator in these next-generation AI infrastructure deployments. Though we remain laser focused on AI platforms, we continue to see large and growing market opportunities within the general-purpose compute space for our PCIe, Ethernet and CXL product families. While the transition to faster bandwidth requirements within general purpose computing trails the leading adoption across AI systems, the market size remains substantial. Our Aries business within general purpose compute is poised to benefit from the transition of PCIe peripherals to Gen 5 speeds and the introduction of new CPU generations from Intel and AMD. We are also ramping volume production of our Taurus SCMs for front end networking across hyperscaler general-purpose server platforms. We expect to see broadening adoption of Leo CXL memory controllers across the ecosystem as CXL capable server CPUs are deployed in new cloud infrastructure over the coming years. In summary, we believe Astera Labs has entered a new growth phase and we are well positioned to outpace industry growth rates through a combination of strong secular tailwinds and the expansion of our silicon content opportunity in AI and general-purpose cloud platforms. We have become a trusted and strategic partner to our customers with over 10 million smart connectivity solutions widely deployed and field tested across nearly all major AI infrastructure programs globally. The introduction of the Scorpio Smart Fabric Switch family and our strategic involvement with the UALink standard for scale-up connectivity is the next critical step in our corporate journey. We are hard at work collaborating with our partners to identify and develop new technologies that will expand Astera's footprint from retimer solutions for connectivity or copper within the rack to fabric and optical solutions that connect AI accelerators across the data center. While we have come a long way, it's a remarkable sense of urgency and energy within the company for the opportunities that lay ahead. With that, I will turn the call over to our CFO, Mike Tate, who will discuss our Q3 financial results and our Q4 outlook.\nMike Tate: Thanks, Sanjay, and thanks to everyone for joining the call. This overview of our Q3 financial results and Q4 guidance will be on a non-GAAP basis. The primary difference in Astera Labs non-GAAP metrics is stock-based compensation and its related income tax effects. Please refer to today's press release available on the Investor Relations section of our website for more details on both our GAAP and non-GAAP Q4 financial outlook as well as a reconciliation of our GAAP to non-GAAP financial measures presented on this call. For Q3 of 2024, Astera Labs delivered record quarterly revenue of $113.1 million, which was up 47% versus the previous quarter and 206% higher than the revenue in Q3 of 2023. During the quarter, we shipped products to all major hyperscalers and AI accelerator manufacturers, and we recognize revenue across all four of our product families. Our Aries product family continues to be our largest sales contributor and helped drive the upside in our revenues this quarter. Aries revenues are being driven by continued momentum with third-party GPU-based AI platforms as well as strong ramps on new platforms, featuring internally developed AI accelerators from multiple hyperscaler customers. Also, in Q3, Taurus revenue started to diversify beyond 200 gig applications with an initial production ramp of our 400-gig Ethernet-based systems which are designed into both AI and general-purpose compute systems. Q3 Leo CXL revenues continue to be driven by our customers' purchasing preproduction volumes for the development of their next generation CXL capable compute platforms. Lastly, we began to ship preproduction volumes of our recently announced Scorpio Smart Fabric Switch family during Q3. Q3 non-GAAP gross margins was 77.8% and was down 20 basis points compared with 78% in Q2 of 2024 and better than our guidance of 75%, driven by higher sales volume and a favourable product mix. Non-GAAP operating expenses for Q3 were $51.2 million, up from $41.1 million in the previous quarter, driven by greater-than-expected hiring conversion during the quarter as we aggressively pushed to support additional commercial opportunities. Within Q3 non-GAAP operating expenses, R&D expenses was $36 million, sales and marketing expenses were $7 million and general and administrative expenses was $8.3 million. Non-GAAP operating margin for Q3 was 32.4%, up from 24.4% in Q2, and demonstrates strong operating leverage as revenue growth outpaced higher operating expenses. Interest income in Q3 was $10.9 million. Our non-GAAP tax provision was $7.3 million for the quarter, which represented a tax rate of 15% on a non-GAAP basis. Non-GAAP fully diluted share count for Q3 was 173.8 million shares, and our non-GAAP diluted earnings per share for the quarter was $0.23. Cash flow from operating activities for Q3 was $63.5 million and we ended the quarter with cash, cash equivalents and marketable securities of $887 million. Now turning to our guidance for Q4 of fiscal 2024. We expect Q4 revenue to increase to within a range of $126 million and $130 million, up roughly 11% to 15% from the prior quarter. For Q4, we expect continued strong growth from our Aries product family across a diverse set of AI platforms, some of which are just starting to ramp and also from our Taurus SCM for 400-gig applications, and additional preproduction shipments of our Scorpio P-Series switches. We expect non-GAAP gross margins to be approximately 75%. The sequential decline in gross margin is driven by an expected product mix shift towards hardware solutions during the quarter. We expect non-GAAP operating expenses to be in a range of approximately $54 million to $55 million as we continue to expand our R&D resource pool across headcount and intellectual property. Interest income is expected to be approximately $10 million. Our non-GAAP tax rate should be approximately 10% and our non-GAAP fully diluted share count is expected to be approximately 179 million shares. Adding this all up, we are expecting non-GAAP fully diluted earnings per share of a range of approximately $0.25 and to $0.26. This concludes our prepared remarks. And once again, we appreciate everyone joining the call. And now we will open the line up for questions. Operator?\nOperator: All right. Thank you. [Operator Instructions]. Our first question comes from Harlan Sur from JPMorgan. Please go ahead.\nHarlan Sur: Good afternoon. And congratulations on the strong results. On your core, Retimer business looks like very strong this year, looks strong next year. Majority of the XPU shipments are still going to be I think Gen 5 based where your market share is still somewhere in the range of, I think, like 95%. And your customers, both merchant and ASIC are ramping new SKUs starting second half of this year, first half of next year. We've also got the ramp of your Gen 6 products, Retimers, Scorpio Switch products with your lead GPU customer in which they're ramping now, but AEC firing, SCM is firing. So given all of this activity, I assume your visibility and backlog is quite strong. Maybe if you can just qualitatively sort of articulate your confidence on driving sequential growth over the next several quarters or at least visibility for a strong first half of next year versus second half of this year?\nSanjay Gajendra: Yes. Thanks, Harlan. Yes, right now, our visibility is very strong, both as always with our backlog position, but also the breadth of designs we have -- right now, we're really kind of entering a new phase of growth here where our revenue streams are clearly diversifying. If you look at right now versus a year ago, we're very excited about the breadth of designs and product lines that we're ramping in. So in the back half of this year, obviously, Taurus has been very incremental, and that continues in Q4 and also the programs that we're ramping on Taurus are just getting started. So we have good confidence that Taurus will continue to grow nicely into 2025. Aries, as you alluded to, the Gen 5 story is still -- got a lot of legs. We have both with the merchant GPUs, but also on these internal ASIC designs, a lot of those products are just starting. And with the Gen 5, we have both the front-end connectivity and the back-end connectivity. Incrementally, Gen 6 will start to play out as well. And with Gen 6, we get an ASP boost on top of that. And then finally, with Leo, we've been talking about Leo for quite a while, but these are new types of technologies that are being adopted in the marketplace, and we're happy to report that we do have line of sight to our first production shipments starting in the middle of the year next year.\nHarlan Sur: Perfect. Thank you, for that. And then on the early traction you're getting with your Scorpio switch portfolio, team has talked on some of the performance benefits, both at the silicon level and chip level, but how much of the differentiator is your COSMOS software stack that your customers have already built into their data center management systems adopted it with your Aries Retimer products, and now you've got the same software stack, you've integrated into your switch products that enables telemetry, linked performance, linked monitoring, tuning up parameters, all that kind of stuff, which is so critical for data center managers. But is the software stack familiarity, current adoption, sort of a big differentiator on the traction with Scorpio?\nJitendra Mohan: Harlan, this is Jiten. And you're absolutely right. There are several things that differentiate Scorpio family. First and foremost, I would say that we built this ground up from for AI applications. If you think about historical deployments of PCI Express switches, they were generally built for server applications for storage and things like that, which are quite different from AI. So we designed the chip for AI applications. We put in the performance that's required for these AI applications. In addition, even the form factor has been designed for AI -- so rather than building a large switch, we ended up building a smaller device such that you are not running these high-speed signals all over the board. So all of that is very purpose-built for AI. Now to your point about software, -- as you remember, our chips are architecture with a software-first approach. So we can deliver a lot of customization based on COSMOS, which is something that our hyperscaler customers are looking for, especially for the X family, which is deployed in a more homogeneous GP to GPU connectivity. Where the Scorpio family sits, we have access to a lot more diagnostic information. And we can couple that with the information that we are collecting from our other families deployed such as Aries and even Taurus to provide a holistic view of the AI infrastructure to the data center operators. So both from the hardware side, the kind of the purpose-built nature of these devices as well as the software stack that comes with it is a big differentiator for us.\nHarlan Sur: Very insightful. Thank you.\nOperator: Our next question comes from the line of Joe Moore from Morgan Stanley. Please go ahead.\nJoe Moore: Great. Thank you. And congratulations on the numbers. I wonder if you could talk about the Scorpio business in terms of -- you gave a number for 2028. Can you give us a sense for what that ramp looks like? Any kind of qualitative discussion of how big it could be in calendar '25, would be very helpful. Thank you.\nSanjay Gajendra: Yes. Thanks, Joe. Yes. What's exciting is, since our public release, we're getting a lot of incremental excitement for the customer base. And what's really neat about the Scorpio product family is a diversity of designs that we're seeing. So clearly, being first with the Gen 6, there's a lot of interest in that application. But there's still a lot of Gen 5 opportunities that are developing that we're addressing as well because with a Gen 6 capable switch, it's backwards compatible. So both design opportunities are open for us. And then incrementally, we have the X-Series that does the back-end connectivity, and that's kind of a greenfield opportunity as well. So these designs are generally a little more customized systems. So the bring up in the qualification cycle is a little bit a lot more long. So we -we take a conservative view on how they ramp like we always do with most of our business. But overall, given that we expect Scorpio to exceed 10% of our revenues in 2025. And as the deployments get into production during the course of the year and exit the year at a very good run rate and good momentum going into 2026.\nJoe Moore: That's great. Thank you. And then on Leo, you talked about some of the ramps there. I guess this application, in particular, of these large memory servers being able to actually reduce the CPU count and maintain this high-memory bandwidth. Can you just talk about that application? And are you seeing that as a material factor next year as well?\nSanjay Gajendra: Yes. So Joe, this is Sanjay here. Yes. So I think like we have been maintaining with any new technology, it takes a little bit of time for things to mature. So right now, the way we look at CXL is it's a transition from the sort of the crawl stage to walk stage. There are three or four key use cases that have emerged. One of them is what you noted, which is the large in-memory database applications. And there, the use case becomes one of how do you enable more memory capacity. In the past, this was done by adding additional CPUs into the server box to provide for more memory channels. But what we have demonstrated is that by using Leo you're not only able to get the higher performance by the added memory. But from an overall TCO standpoint, it's significantly less than adding additional CPUs. So that's one key use case that we see from a deployment standpoint. But having said that I think at OCP this year, you might have seen some of our videos and all that, there's been tremendous amount of different platform level solutions that were being deployed for various high bandwidth applications, HPC applications, including some of the rack level disaggregation type of use cases. So to that standpoint, there are many different ways in which the technology can develop, but just like any new technology, it will take some time before the requisite ecosystem and software is built. So we are in that period right now, getting those pieces in place and 2025 is when we expect the production volumes to begin.\nJoe Moore: Great. Thank you.\nOperator: Our next question comes from the line of Blayne Curtis from Jefferies. Please go ahead.\nBlayne Curtis: Hi, good afternoon and congrats on the results. I wanted to ask you last quarter you kind of talked about the September growth. I think it was like 20 million and you kind of loosely said it was kind of one-third Retimers, one-third of the PCIe cabling and one-third Taurus. So I'm not expecting to dial us in completely, but you kind of double that. So I'm kind of just curious, the strength you're seeing between your products, if you could add a little bit more color. And then also just between AI accelerators and GPUs, obviously, the big GPU vendor has a ramp coming with Blackwell, but that's not exactly now. So I'm just kind of curious what's driving the strength in September and December a little bit more?\nMike Tate: Yes. Thanks, Blayne. Yes. In Q3, our business is really benefiting from the strong contribution of multiple product lines. And the Aries SCM and Taurus both was really big, strong ramp quarters. Those ramps kind of were consistent with our expectations. The upside to the guidance was driven largely from Aries' revenue, both for the third-party GPUs, but also as well with the strong ramps on new platforms on the internally developed AI accelerators. And we're seeing that across multiple hyperscaler customers, so it's not just one. So the upside was largely driven by that Aries revenue.\nBlayne Curtis: Thanks. And then, Mike, I want to ask you on with the addition of the Scorpio product line, but before you kind of talked about how -- when some of the products like Taurus or the PCI modules ramp, it would be a little bit dilutive to margins because it's not a chip sale. How do you think about it if you do have switches greater than 10%, maybe switch margins versus Aries? How do we think of that blend next year?\nMike Tate: Yes. So for Scorpio, there will be a broader range of margins. There's different use cases. So it depends on the functionality and the volume. But at this point, we believe Scorpio will not impact our long-term gross margin targets of 70% and it was kind of contemplated when we set up those targets. I'd say overall, beyond just Scorpio though we are seeing a wider range of margin profiles across all our product portfolio. So mix will be important to contribute from a quarter-to-quarter perspective. But we still feel good about the 70% target.\nBlayne Curtis: Thanks, Mike.\nOperator: Our next question comes from the line of Thomas O'Malley from Barclays. Please go ahead.\nThomas O'Malley: Hi, guys. Thanks for taking my questions. My first one is just on the X-Series for Scorpio. I think this is the first real kind of backend switch that we've seen in the marketplace for PCIe. Could you talk about your positioning there? How far you think you're ahead? And would you expect the same kind of competitive dynamic that you're seeing in the P-Series switch? Just talk about where you are competitively and just from an opportunity perspective, do you think longer term, the X Series is a bigger opportunity than the P-Series?\nJitendra Mohan: Tom, let me take the question. This is Jitendra. So you have three questions in there. So you don't get a follow-up. So the first question, let me ask you the P-series first. So P-series is actually broadly applicable to all of the accelerators. All of the accelerators require connectivity from the GPU or the accelerator to the head node or to the NIX and so on. So P-series is applicable to all of them. The P family for our supports PCI Express Gen 6. So that's where the deployment will happen. I already mentioned what are some of the advantages of the family, but at the same time, we should not estimate the Gen 5 socket. There are also Gen 5 designs that are taking place right now, both with the third-party GPUs as well as with ASIC, internally developed ASIC. So I think that's the market opportunity we see with the P-Series. We estimate that the TAM for this P-Series to be about $1 billion plus or minus today and growing to double of that over time. But you're also correct that we do think over the longer period, X series will have a bigger TAM. The TAM today is nearly zero. It's not very commonly used outside of the Nvidia ecosystem. We do expect many hyperscalers to start deploying this, starting with the X family and the designs for which that we have. And we are able to do that because of the architecture that we have. Because of our software-defined architecture, we can customize many parts of the X-Series to cater to the specific requirements of the hyperscalers both on the side of performance, the exact configuration that they require in count and so on and also the diagnostics framework that they require to monitor their infrastructure. So over time, we do expect X-Series to become larger. Now I also mentioned during the remarks that we have joined the UALink consortium and that gives us another market, another opportunity where we can play with back-end interconnect.\nThomas O'Malley: Helpful. Let me sneak in another one. I know I broke the rules there on the first one. But the second is just on the Taurus product family. So historically, you've been concentrated within one customer. Can you talk about the breadth of your engagements there as that kind of expanded to multiple customers? And when you look out into next year is that going to be largely consolidated to one or two customers or do you see that kind of proliferating across multiple?\nSanjay Gajendra: Yes. This is Sanjay here. Let me take that. Yes. So 2025 is a year that we think the business will get broader. As we've always said, AEC or active electrical cable is a case-by-case situation, meaning it's not like every hyperscaler uses active electrical cables. So to that standpoint, we do expect as data rates go higher with 800 gig and so on, that market to be much more diversified than what it is today. So with that said, I mean, today, if you look at our business, we do have our AEC modules or Taurus modules going into both AI and compute platforms. There are different kinds of cables in terms of various different configurations that they need to support. So overall, I want to say we are fairly diversified with our business today, but as the speeds increase, in 2025 and beyond. We do expect that the customer bases will continue to evolve. With the note like that I highlighted that every infrastructure will be different the place where AEC would be used will differ between the various hyperscalers.\nOperator: Our next question comes from the line of Tore Svanberg from Stifel. Please go ahead.\nTore Svanberg: Yes. Thank you and congrats on the strong results. You mentioned that PCIe Gen 6 is going to be in preproduction this quarter. When do you expect it to be an actual volume production? Would that be in the first half of next year, Q1?\nSanjay Gajendra: I want to say it depends on the customers' timeline. So we don't want to speak for any of our customers on what they are communicating. But in general, what I would say though, similar to what we've shared with you in the past that our design wins are in the customized rack implementation. So to that standpoint, the timing of qualification and the deployment would be based on that. But in general between the initial design wins we had to where we are now where we are engaging with multiple opportunities, both for Gen 5 and Gen 6. And both for third-party GPUs as well as internally developed GPUs. So to that standpoint, our opportunities on Scorpio continues to grow. And as you look at overall for 2025 like Mike suggested, we do expect our contribution from Scorpio to be in excess of 10% of our overall revenue.\nTore Svanberg: That's very helpful. And I had a follow-up question on Scorpio in relation to your PCIe Retimer business. So would those typically pull each other, meaning -- could it be instances where there's a switch, a PCIe switch with somebody else's Retimer or I mean, do they pretty much go hand in hand, especially given the customer software?\nSanjay Gajendra: Yes. So if you look at today's deployment with Gen 5, at least from an industry snapshot standpoint, it's mix and match, right, meaning our Retimers get used with switch products from other vendors. We have gone -- because of our software-based architecture, it allows us to uniquely customize and optimize for different system-level configurations. So that is what it is today. Going forward, with COSMOS, we do see an advantage because we have integrated the management framework, the customization framework and the optimization type of feature set across all of our products. Meaning if a customer is using COSMOS today for Aries, they will very easily be able to extend the infrastructure that they've already built to run on top of our Scorpio devices. That's a unique advantage we bring compared to some of the alternatives out there.\nTore Svanberg: Very helpful. Thank you.\nOperator: Our next question comes from Ross Seymore from Deutsche Bank. Please go ahead.\nRoss Seymore: Hi, thanks. So, yes, a couple of questions, and congrats on the strong results. The first question, Jitendra, you mentioned and Sanjay you too, about the importance of the ASIC side of the business really ramping up strongly. What was the inflection point that's really driving that that's something where the market itself is just getting stronger? Or is there something with the inflection point of your technology that's being adopted and penetrating the market faster?\nSanjay Gajendra: Yes. So I think in terms of the ASIC designs, I think it's fairly public knowledge now that all the hyperscalers have doubled down on the amount of investment they're doing for their own internal ASIC programs. The third-party GPUs, obviously, have done a great job, but also hyperscalers are starting to realize where the money is in terms of their AI use cases and workloads. So to that standpoint, we have been seeing an increased investment from hyperscalers in terms of their internal programs. And we are, of course, addressing those across all of our product lines. So if you look at our business today, like we highlighted in the prepared remarks, we have truly entered a new phase in terms of our overall business, where we not only have the third-party GPU-based designs, we also have several internal AI internally developed accelerator-based AI platforms. And then we have multiple of our product lines that are ramping on these platforms. The one caveat -- one additional point I would note is that for the internally developed AI platforms, we get to play not only in the front-end network, connecting the GPU to CPU and storage, we also get to play in the back end, which generally like -generally tends to be, like I call it, a fertile land where there are multiple connectivity requirements that we can service with our Aries, Taurus product line and now, of course, with the Scorpio X-Series product line.\nRoss Seymore: Thanks for that, and I guess as my one follow-up, a quarter ago, we were having significant debates about your statements about the average content per GPU. Obviously, that's not as big a topic today now that we know about Scorpio with more detail -- but if I revisit that and you talked still on this call about the average content going up, is that just because of Scorpio, something you had in your back pocket before that you obviously couldn't tell us about -- or do you still believe the Retimer content in some way, shape or form will still be bigger on most of these platforms going forward, especially for the third-party merchant suppliers.\nSanjay Gajendra: Yes. So I think when we talked about it before, there were two reasons we highlighted. One is generally speaking with each new generation of a protocol like PCIe going from Gen 5 to Gen 6. There is an ASP uplift. That's number one. Number two, of course, we were hinting at the Scorpio product line, which because of the value it delivers to customers is at a higher ASP, as you can imagine. So overall, if you look at the design wins we have today, the dollar content per GPU goes up, that's one way to look at it based on what we've shared before. The other way to also look at it is that for internally developed platform, we get to play also in terms of the back-end network, like I noted, we get to also address some of the scale-out networks that are based on Ethernet using Taurus module. So overall, if you look at sort of the increasing speed, additional product line as well as the fact that the internally developed platforms, AI accelerated database platforms, they are starting to gain more and more traction. So when you look at all of them, on an average, our content is on the up.\nRoss Seymore: Thank you.\nOperator: Our next question comes from the line of Quinn Bolton from Needham. Please go ahead.\nQuinn Bolton: Hey thanks for squeezing me in. I just wanted to -- just a follow-up clarification maybe. But for the Scorpio family being over 10% of revenues, is that largely from the P-Series? Or would you expect any X Series production revenue in 2025 given the longer, I think, designing cycle for the back-end scale-up networks.\nMike Tate: We have designs for both -- both P and X will contribute to revenues. The P designs will generally be first, but we do see X starting in the back half of the year as well.\nQuinn Bolton: Excellent. And then can you give us some sense for the P-Series and the X Series -- on the Retimer, I think the market sort of generally looked at the Aries Retimer attach rate per GPU or per accelerator. Is there any framework you can provide for us for P-Series X Series, would you look or expect a typical attach rate per GPU accelerator, would that be 1:1? Would it be less than 1:1, could it be higher than 1:1. Any thoughts on attach rates for P-Series and X-Series? Thank you.\nSanjay Gajendra: Yes. So it's a very broad question because there's all kinds of implementations that are out there. To a high level, I'll probably share three points. The first is P Series is broadly applicable. In the sense that it could work for a third-party GPU or an internally developed accelerator because every accelerator doesn't matter where it comes from needs to connect to the head note side, which generally includes the networking, storage as well as CPU. So to that standpoint, that will be a very broadly used device. And when it use, it's 1:1, meaning every GPU would need one of our Scorpio P-Series device. That's number one. Number two is the X Series. Now these are generally used for GPU to GPU interconnect, right? So to that standpoint, depending on the configuration, the number of devices is a function of the number of ports that an X Series device exposes and really depends on how the back-end fabric is built. And to that standpoint, again, it truly depends on how the configuration is being built. And this one, like Mike noted, it's a greenfield use case, meaning if you keep Nvidia and NV Switch aside, everyone else is starting to build configurations that are obviously going to need some kind of a switching functionality, which is what we are addressing with our X Series device. So that's the second point to keep in mind. And then in general, what I would say is that overall, depending on where things are and how big of a chip that we're building, the attach rate will continue to evolve. But in general, the dollar content that we're talking about is expected to continue to grow both because we are adding more functionality with devices like Scorpio. And at the same time, we are seeing additional pull-in for products like Aries and Taurus and other things that we're working on.\nOperator: Our next question comes from the line of Mark Lipacis from Evercore ISI. Please go ahead.\nMark Lipacis: Hi, thanks for taking my question. A question on the diversification. If you think just longer term, you can diversify by your customer base and then by your product line. So pick your time in the future three years out, five years out, what do you think your split will be between the merchant GPU player versus the custom AI accelerator player like how your product will be attached to either type of solution? And then let's just say, three years out, you have five product lines. Is that -- would you expect to have still have a skew to one? Or would you expect to have like 20%, in each product line bucket or something like that? If you could help us out how you're thinking about diversification like three years out? I think that would be helpful. And then I have a follow-up. Thanks.",
        "speaker1": {
            "name": "Mike Tate",
            "content": "Thanks, Sanjay, and thanks to everyone for joining the call. This overview of our Q3 financial results and Q4 guidance will be on a non-GAAP basis. The primary difference in Astera Labs non-GAAP metrics is stock-based compensation and its related income tax effects. Please refer to today's press release available on the Investor Relations section of our website for more details on both our GAAP and non-GAAP Q4 financial outlook as well as a reconciliation of our GAAP to non-GAAP financial measures presented on this call. For Q3 of 2024, Astera Labs delivered record quarterly revenue of $113.1 million, which was up 47% versus the previous quarter and 206% higher than the revenue in Q3 of 2023. During the quarter, we shipped products to all major hyperscalers and AI accelerator manufacturers, and we recognize revenue across all four of our product families. Our Aries product family continues to be our largest sales contributor and helped drive the upside in our revenues this quarter. Aries revenues are being driven by continued momentum with third-party GPU-based AI platforms as well as strong ramps on new platforms, featuring internally developed AI accelerators from multiple hyperscaler customers. Also, in Q3, Taurus revenue started to diversify beyond 200 gig applications with an initial production ramp of our 400-gig Ethernet-based systems which are designed into both AI and general-purpose compute systems. Q3 Leo CXL revenues continue to be driven by our customers' purchasing preproduction volumes for the development of their next generation CXL capable compute platforms. Lastly, we began to ship preproduction volumes of our recently announced Scorpio Smart Fabric Switch family during Q3. Q3 non-GAAP gross margins was 77.8% and was down 20 basis points compared with 78% in Q2 of 2024 and better than our guidance of 75%, driven by higher sales volume and a favourable product mix. Non-GAAP operating expenses for Q3 were $51.2 million, up from $41.1 million in the previous quarter, driven by greater-than-expected hiring conversion during the quarter as we aggressively pushed to support additional commercial opportunities. Within Q3 non-GAAP operating expenses, R&D expenses was $36 million, sales and marketing expenses were $7 million and general and administrative expenses was $8.3 million. Non-GAAP operating margin for Q3 was 32.4%, up from 24.4% in Q2, and demonstrates strong operating leverage as revenue growth outpaced higher operating expenses. Interest income in Q3 was $10.9 million. Our non-GAAP tax provision was $7.3 million for the quarter, which represented a tax rate of 15% on a non-GAAP basis. Non-GAAP fully diluted share count for Q3 was 173.8 million shares, and our non-GAAP diluted earnings per share for the quarter was $0.23. Cash flow from operating activities for Q3 was $63.5 million and we ended the quarter with cash, cash equivalents and marketable securities of $887 million. Now turning to our guidance for Q4 of fiscal 2024. We expect Q4 revenue to increase to within a range of $126 million and $130 million, up roughly 11% to 15% from the prior quarter. For Q4, we expect continued strong growth from our Aries product family across a diverse set of AI platforms, some of which are just starting to ramp and also from our Taurus SCM for 400-gig applications, and additional preproduction shipments of our Scorpio P-Series switches. We expect non-GAAP gross margins to be approximately 75%. The sequential decline in gross margin is driven by an expected product mix shift towards hardware solutions during the quarter. We expect non-GAAP operating expenses to be in a range of approximately $54 million to $55 million as we continue to expand our R&D resource pool across headcount and intellectual property. Interest income is expected to be approximately $10 million. Our non-GAAP tax rate should be approximately 10% and our non-GAAP fully diluted share count is expected to be approximately 179 million shares. Adding this all up, we are expecting non-GAAP fully diluted earnings per share of a range of approximately $0.25 and to $0.26. This concludes our prepared remarks. And once again, we appreciate everyone joining the call. And now we will open the line up for questions. Operator? Yes. Thanks, Blayne. Yes. In Q3, our business is really benefiting from the strong contribution of multiple product lines. And the Aries SCM and Taurus both was really big, strong ramp quarters. Those ramps kind of were consistent with our expectations. The upside to the guidance was driven largely from Aries' revenue, both for the third-party GPUs, but also as well with the strong ramps on new platforms on the internally developed AI accelerators. And we're seeing that across multiple hyperscaler customers, so it's not just one. So the upside was largely driven by that Aries revenue. Yes. So for Scorpio, there will be a broader range of margins. There's different use cases. So it depends on the functionality and the volume. But at this point, we believe Scorpio will not impact our long-term gross margin targets of 70% and it was kind of contemplated when we set up those targets. I'd say overall, beyond just Scorpio though we are seeing a wider range of margin profiles across all our product portfolio. So mix will be important to contribute from a quarter-to-quarter perspective. But we still feel good about the 70% target. We have designs for both -- both P and X will contribute to revenues. The P designs will generally be first, but we do see X starting in the back half of the year as well."
        },
        "speaker2": {
            "name": "Jitendra Mohan",
            "content": "Thank you, Leslie. Good afternoon, everyone, and thanks for joining our third quarter conference call for fiscal year 2024. Today, I will provide a quick Q3 financial overview, discuss several of our recent company-specific product and strategic announcements, followed by a discussion around the key secular trends driving our market opportunity. I will then turn the call over to Sanjay to delve deeper into our growth strategy. Finally, Mike will provide additional details on our Q3 results and our Q4 financial guidance. Astera Labs delivered strong Q3 results, setting our fifth consecutive record for quarterly revenue at $113 million, which was up 47% from the last quarter and up 206% versus the prior year. Our business has entered a new growth phase with multiple product families ramping across AI platforms, featuring both third-party GPUs and internally developed AI accelerators, which drove the Q3 sales upside versus our guidance. We also demonstrated strong operating leverage during Q3, with non-GAAP operating margin expanding to over 32% and delivered non-GAAP EPS of $0.23. Looking into Q4, we expect our revenue momentum to continue, largely driven by the Aries PCIe and Taurus Ethernet product lines. The Scorpio Fabric Switches is continuing to ship in preproduction volumes. The criticality of connectivity in modern AI clusters continues to grow with trillion parameter model sizes, multistep reasoning models and faster, more complex AI accelerators. These developments present a tremendous opportunity for Astera Labs' intelligent connectivity platform to enhance AI server performance and productivity with our differentiated hardware and software solutions. At the start of Q4, we announced our fourth product line, the Scorpio Smart Fabric Switch family, which expands our mission of solving the increasingly complex connectivity challenges within AI infrastructure, both for scale out and for scale up networks. Extending beyond our current market footprint of PCI Express and Ethernet Retimer class products and controller class devices for CXL memory, our Scorpio Smart Fabric Switch family delivers meaningfully higher functionality and value to our AI and cloud infrastructure customers. We estimate that Scorpio will expand our total market opportunity for our four product families to more than $12 billion by 2028. Scorpio family unlocks a large and growing opportunity across AI Server Head Node scale-out applications with the P-Series and AI accelerators scale up clustering use cases with the X-Series. The P-Series devices directly address the challenge of keeping modern GPUs fed with data at ever-increasing speeds while the X-Series devices improve the efficiency and size of AI clusters. The Scorpio family was purpose-built from the ground up for these AI-specific workloads with close alignment with our hyperscaler and AI platform partners. At the recent 2024 OCP Global Summit, we demonstrated the industry's first PCIe Gen 6 fabric switch, which is currently shipping in preproduction volumes for AI platforms. We are happy to report that we already have design wins of both Scorpio P-Series and X-Series and that our recent product launches further accelerated strong customer and ecosystem interest. Over the coming quarters, we expect to further expand our business opportunities for the Scorpio product family across PCIe Gen 5, PCIe Gen 6 and platform-specific customized connectivity platforms. Further expanding our market opportunity, Astera Labs has joined the Ultra Accelerator Link, UALink Consortium as a promoting member on the Board of Directors, along with industry-leading hyperscalers and AI platform providers. This important industry initiative places us at the forefront of developing and advancing an open high-speed, low-latency interconnect for scale-up connectivity between accelerators. Astera Labs' deep expertise in developing advanced silicon-based connectivity solutions, along with a strong track record of technology execution makes us uniquely suited to contribute to this compelling and necessary industry initiative. We will shift towards shorter AI platform refresh cycles. Hyperscalers are increasingly relying on their trusted partners as they deploy new hardware in their data center infrastructure at an unprecedented pace and scale. To date, we have demonstrated strong execution with our Aries, Taurus, Leo and now Scorpio product family. Our products increase data, networking, memory bandwidth and capacity and our COSMOS software provides our hyperscaler customers with the tools necessary to monitor and observe the health of their expensive infrastructure to ensure maximum system utilization. To conclude, Astera Labs expanding product portfolio, including the new Scorpio Smart Fabric Switches, is cementing our position as a critical part of AI connectivity infrastructure, delivering increased value to our hyperscaler customers and unlocking additional multiyear growth trajectories for Astera Labs. With that, let me turn the call over to our President and COO, Sanjay Gajendra, to discuss some of our recent product announcements and our long-term growth strategy. Harlan, this is Jiten. And you're absolutely right. There are several things that differentiate Scorpio family. First and foremost, I would say that we built this ground up from for AI applications. If you think about historical deployments of PCI Express switches, they were generally built for server applications for storage and things like that, which are quite different from AI. So we designed the chip for AI applications. We put in the performance that's required for these AI applications. In addition, even the form factor has been designed for AI -- so rather than building a large switch, we ended up building a smaller device such that you are not running these high-speed signals all over the board. So all of that is very purpose-built for AI. Now to your point about software, -- as you remember, our chips are architecture with a software-first approach. So we can deliver a lot of customization based on COSMOS, which is something that our hyperscaler customers are looking for, especially for the X family, which is deployed in a more homogeneous GP to GPU connectivity. Where the Scorpio family sits, we have access to a lot more diagnostic information. And we can couple that with the information that we are collecting from our other families deployed such as Aries and even Taurus to provide a holistic view of the AI infrastructure to the data center operators. So both from the hardware side, the kind of the purpose-built nature of these devices as well as the software stack that comes with it is a big differentiator for us. Tom, let me take the question. This is Jitendra. So you have three questions in there. So you don't get a follow-up. So the first question, let me ask you the P-series first. So P-series is actually broadly applicable to all of the accelerators. All of the accelerators require connectivity from the GPU or the accelerator to the head node or to the NIX and so on. So P-series is applicable to all of them. The P family for our supports PCI Express Gen 6. So that's where the deployment will happen. I already mentioned what are some of the advantages of the family, but at the same time, we should not estimate the Gen 5 socket. There are also Gen 5 designs that are taking place right now, both with the third-party GPUs as well as with ASIC, internally developed ASIC. So I think that's the market opportunity we see with the P-Series. We estimate that the TAM for this P-Series to be about $1 billion plus or minus today and growing to double of that over time. But you're also correct that we do think over the longer period, X series will have a bigger TAM. The TAM today is nearly zero. It's not very commonly used outside of the Nvidia ecosystem. We do expect many hyperscalers to start deploying this, starting with the X family and the designs for which that we have. And we are able to do that because of the architecture that we have. Because of our software-defined architecture, we can customize many parts of the X-Series to cater to the specific requirements of the hyperscalers both on the side of performance, the exact configuration that they require in count and so on and also the diagnostics framework that they require to monitor their infrastructure. So over time, we do expect X-Series to become larger. Now I also mentioned during the remarks that we have joined the UALink consortium and that gives us another market, another opportunity where we can play with back-end interconnect.\nThomas O'Malley: Helpful. Let me sneak in another one. I know I broke the rules there on the first one. But the second is just on the Taurus product family. So historically, you've been concentrated within one customer. Can you talk about the breadth of your engagements there as that kind of expanded to multiple customers? And when you look out into next year is that going to be largely consolidated to one or two customers or do you see that kind of proliferating across multiple?"
        },
        "speaker3": {
            "name": "Sanjay Gajendra",
            "content": "Thanks, Jitendra, and good afternoon, everyone. We are pleased with our Q3 results and strong financial outlook for Q4. Overall, we believe we have entered a new phase at the company based on two key factors; first is the increased diversity of our business. In 3Q, our business diversified significantly with new product lines and form factors going to high-volume production. We also started ramping on multiple new AI platforms based on internally developed AI accelerators at multiple customers to go along with the continued momentum with third party GPU-based AI platforms. These together helped achieve a record sequential growth in 3Q. Second, with the introduction of Scorpio Smart Fabric Switches, our market opportunity has significantly expanded. This new product line delivers increased value to our hyperscaler customers and for Astera Labs unlocks higher dollar content in AI platforms, an additional multiyear growth trajectories. Let me explain our business drivers in more detail. As noted, we now have multiple product lines generations and form factors in high-volume production. This includes Aries Smart DSP Retimers and smart cable modules for PCIe 5.0 and Taurus smart cable modules for 200 and 400-gig Ethernet active electrical cables. We are also shipping preproduction volumes for Leo CXL memory controller\u2019s times for PCIe 6.0 and Scorpio fabric switch solutions for PCIe head node connectivity. All these new products deliver increased value to our customers and therefore, command higher ASPs. Our first market Scorpio PCIe Gen 6 fabric switch addresses a multibillion-dollar opportunity with a ground-up architecture designed for AI data flows and delivers maximum predictable performance per watt in mixed-mode PCIe head note connectivity compared to incumbent solutions. We are currently shipping Scorpio P-Series in preproduction quantities to support qualification for customized AI platforms based upon leading third-party GPUs. Interest for Scorpio P-Series has accelerated since the former launch given its differentiated features and as a result, we are engaging in multiple design opportunities across a diverse spectrum of AI platforms. These include both PCIe Gen 6 and PCIe Gen 5 implementations on third-party GPU and internal accelerator-based platforms. Overall, we are very bullish on the market for our entire product portfolio across third-party GPU-based systems with increasing dollar content per GPU on our new design wins and we believe that Astera's opportunity with internally developed AI accelerator platforms can be even larger with opportunities both in the scale-out and back-end scale-up clustering use cases. This additional market opportunity has unlocked design activity for Aries and Taurus product lines for reach extension within and between racks and for our newly introduced Scorpio X-Series Fabric Switches for homogeneous accelerator to accelerator connectivity maximum bandwidth. The Scorpio X-Series is built upon our software-defined architecture and leverages our cost more software suite to support a variety of platform-specific customization which provide hyperscalers with valuable flexibility. As we look ahead to 2025, we will begin to ramp designs across new internally developed AI accelerator-based platforms that will incorporate multiple Astera Labs product families, including Aries, Taurus and Scorpio. As a result, we will continue to benefit from increased dollar content per accelerator in these next-generation AI infrastructure deployments. Though we remain laser focused on AI platforms, we continue to see large and growing market opportunities within the general-purpose compute space for our PCIe, Ethernet and CXL product families. While the transition to faster bandwidth requirements within general purpose computing trails the leading adoption across AI systems, the market size remains substantial. Our Aries business within general purpose compute is poised to benefit from the transition of PCIe peripherals to Gen 5 speeds and the introduction of new CPU generations from Intel and AMD. We are also ramping volume production of our Taurus SCMs for front end networking across hyperscaler general-purpose server platforms. We expect to see broadening adoption of Leo CXL memory controllers across the ecosystem as CXL capable server CPUs are deployed in new cloud infrastructure over the coming years. In summary, we believe Astera Labs has entered a new growth phase and we are well positioned to outpace industry growth rates through a combination of strong secular tailwinds and the expansion of our silicon content opportunity in AI and general-purpose cloud platforms. We have become a trusted and strategic partner to our customers with over 10 million smart connectivity solutions widely deployed and field tested across nearly all major AI infrastructure programs globally. The introduction of the Scorpio Smart Fabric Switch family and our strategic involvement with the UALink standard for scale-up connectivity is the next critical step in our corporate journey. We are hard at work collaborating with our partners to identify and develop new technologies that will expand Astera's footprint from retimer solutions for connectivity or copper within the rack to fabric and optical solutions that connect AI accelerators across the data center. While we have come a long way, it's a remarkable sense of urgency and energy within the company for the opportunities that lay ahead. With that, I will turn the call over to our CFO, Mike Tate, who will discuss our Q3 financial results and our Q4 outlook. Yes. Thanks, Harlan. Yes, right now, our visibility is very strong, both as always with our backlog position, but also the breadth of designs we have -- right now, we're really kind of entering a new phase of growth here where our revenue streams are clearly diversifying. If you look at right now versus a year ago, we're very excited about the breadth of designs and product lines that we're ramping in. So in the back half of this year, obviously, Taurus has been very incremental, and that continues in Q4 and also the programs that we're ramping on Taurus are just getting started. So we have good confidence that Taurus will continue to grow nicely into 2025. Aries, as you alluded to, the Gen 5 story is still -- got a lot of legs. We have both with the merchant GPUs, but also on these internal ASIC designs, a lot of those products are just starting. And with the Gen 5, we have both the front-end connectivity and the back-end connectivity. Incrementally, Gen 6 will start to play out as well. And with Gen 6, we get an ASP boost on top of that. And then finally, with Leo, we've been talking about Leo for quite a while, but these are new types of technologies that are being adopted in the marketplace, and we're happy to report that we do have line of sight to our first production shipments starting in the middle of the year next year. Yes. Thanks, Joe. Yes. What's exciting is, since our public release, we're getting a lot of incremental excitement for the customer base. And what's really neat about the Scorpio product family is a diversity of designs that we're seeing. So clearly, being first with the Gen 6, there's a lot of interest in that application. But there's still a lot of Gen 5 opportunities that are developing that we're addressing as well because with a Gen 6 capable switch, it's backwards compatible. So both design opportunities are open for us. And then incrementally, we have the X-Series that does the back-end connectivity, and that's kind of a greenfield opportunity as well. So these designs are generally a little more customized systems. So the bring up in the qualification cycle is a little bit a lot more long. So we -we take a conservative view on how they ramp like we always do with most of our business. But overall, given that we expect Scorpio to exceed 10% of our revenues in 2025. And as the deployments get into production during the course of the year and exit the year at a very good run rate and good momentum going into 2026. Yes. So Joe, this is Sanjay here. Yes. So I think like we have been maintaining with any new technology, it takes a little bit of time for things to mature. So right now, the way we look at CXL is it's a transition from the sort of the crawl stage to walk stage. There are three or four key use cases that have emerged. One of them is what you noted, which is the large in-memory database applications. And there, the use case becomes one of how do you enable more memory capacity. In the past, this was done by adding additional CPUs into the server box to provide for more memory channels. But what we have demonstrated is that by using Leo you're not only able to get the higher performance by the added memory. But from an overall TCO standpoint, it's significantly less than adding additional CPUs. So that's one key use case that we see from a deployment standpoint. But having said that I think at OCP this year, you might have seen some of our videos and all that, there's been tremendous amount of different platform level solutions that were being deployed for various high bandwidth applications, HPC applications, including some of the rack level disaggregation type of use cases. So to that standpoint, there are many different ways in which the technology can develop, but just like any new technology, it will take some time before the requisite ecosystem and software is built. So we are in that period right now, getting those pieces in place and 2025 is when we expect the production volumes to begin. Yes. This is Sanjay here. Let me take that. Yes. So 2025 is a year that we think the business will get broader. As we've always said, AEC or active electrical cable is a case-by-case situation, meaning it's not like every hyperscaler uses active electrical cables. So to that standpoint, we do expect as data rates go higher with 800 gig and so on, that market to be much more diversified than what it is today. So with that said, I mean, today, if you look at our business, we do have our AEC modules or Taurus modules going into both AI and compute platforms. There are different kinds of cables in terms of various different configurations that they need to support. So overall, I want to say we are fairly diversified with our business today, but as the speeds increase, in 2025 and beyond. We do expect that the customer bases will continue to evolve. With the note like that I highlighted that every infrastructure will be different the place where AEC would be used will differ between the various hyperscalers. I want to say it depends on the customers' timeline. So we don't want to speak for any of our customers on what they are communicating. But in general, what I would say though, similar to what we've shared with you in the past that our design wins are in the customized rack implementation. So to that standpoint, the timing of qualification and the deployment would be based on that. But in general between the initial design wins we had to where we are now where we are engaging with multiple opportunities, both for Gen 5 and Gen 6. And both for third-party GPUs as well as internally developed GPUs. So to that standpoint, our opportunities on Scorpio continues to grow. And as you look at overall for 2025 like Mike suggested, we do expect our contribution from Scorpio to be in excess of 10% of our overall revenue. Yes. So if you look at today's deployment with Gen 5, at least from an industry snapshot standpoint, it's mix and match, right, meaning our Retimers get used with switch products from other vendors. We have gone -- because of our software-based architecture, it allows us to uniquely customize and optimize for different system-level configurations. So that is what it is today. Going forward, with COSMOS, we do see an advantage because we have integrated the management framework, the customization framework and the optimization type of feature set across all of our products. Meaning if a customer is using COSMOS today for Aries, they will very easily be able to extend the infrastructure that they've already built to run on top of our Scorpio devices. That's a unique advantage we bring compared to some of the alternatives out there. Yes. So I think in terms of the ASIC designs, I think it's fairly public knowledge now that all the hyperscalers have doubled down on the amount of investment they're doing for their own internal ASIC programs. The third-party GPUs, obviously, have done a great job, but also hyperscalers are starting to realize where the money is in terms of their AI use cases and workloads. So to that standpoint, we have been seeing an increased investment from hyperscalers in terms of their internal programs. And we are, of course, addressing those across all of our product lines. So if you look at our business today, like we highlighted in the prepared remarks, we have truly entered a new phase in terms of our overall business, where we not only have the third-party GPU-based designs, we also have several internal AI internally developed accelerator-based AI platforms. And then we have multiple of our product lines that are ramping on these platforms. The one caveat -- one additional point I would note is that for the internally developed AI platforms, we get to play not only in the front-end network, connecting the GPU to CPU and storage, we also get to play in the back end, which generally like -generally tends to be, like I call it, a fertile land where there are multiple connectivity requirements that we can service with our Aries, Taurus product line and now, of course, with the Scorpio X-Series product line. Yes. So I think when we talked about it before, there were two reasons we highlighted. One is generally speaking with each new generation of a protocol like PCIe going from Gen 5 to Gen 6. There is an ASP uplift. That's number one. Number two, of course, we were hinting at the Scorpio product line, which because of the value it delivers to customers is at a higher ASP, as you can imagine. So overall, if you look at the design wins we have today, the dollar content per GPU goes up, that's one way to look at it based on what we've shared before. The other way to also look at it is that for internally developed platform, we get to play also in terms of the back-end network, like I noted, we get to also address some of the scale-out networks that are based on Ethernet using Taurus module. So overall, if you look at sort of the increasing speed, additional product line as well as the fact that the internally developed platforms, AI accelerated database platforms, they are starting to gain more and more traction. So when you look at all of them, on an average, our content is on the up. Yes. So it's a very broad question because there's all kinds of implementations that are out there. To a high level, I'll probably share three points. The first is P Series is broadly applicable. In the sense that it could work for a third-party GPU or an internally developed accelerator because every accelerator doesn't matter where it comes from needs to connect to the head note side, which generally includes the networking, storage as well as CPU. So to that standpoint, that will be a very broadly used device. And when it use, it's 1:1, meaning every GPU would need one of our Scorpio P-Series device. That's number one. Number two is the X Series. Now these are generally used for GPU to GPU interconnect, right? So to that standpoint, depending on the configuration, the number of devices is a function of the number of ports that an X Series device exposes and really depends on how the back-end fabric is built. And to that standpoint, again, it truly depends on how the configuration is being built. And this one, like Mike noted, it's a greenfield use case, meaning if you keep Nvidia and NV Switch aside, everyone else is starting to build configurations that are obviously going to need some kind of a switching functionality, which is what we are addressing with our X Series device. So that's the second point to keep in mind. And then in general, what I would say is that overall, depending on where things are and how big of a chip that we're building, the attach rate will continue to evolve. But in general, the dollar content that we're talking about is expected to continue to grow both because we are adding more functionality with devices like Scorpio. And at the same time, we are seeing additional pull-in for products like Aries and Taurus and other things that we're working on."
        }
    },
    {
        "symbol": "ALAB",
        "quarter": 2,
        "year": 2024,
        "date": "2024-08-07 06:41:09",
        "content": "Operator: Good afternoon. My name is Audra, and I will be your conference operator today. At this time, I would like to welcome everyone to the Astera Labs Q2 2024 Earnings Conference Call. All lines have been placed on mute to prevent any background noise. After managements\u2019 remarks there will be a question-and-answer session. [Operator Instructions] I will now turn the call over to Leslie Green, Investor Relations for Astera Labs. Leslie, you may begin.\nLeslie Green: Good afternoon everyone and welcome to the Astera Labs Second Quarter 2024 Earnings Conference Call. Joining us on the call today are Jitendra Mohan, Chief Executive Officer and Co-Founder; and Sanjay Gajendra, President and Chief Operating Officer and Co-Founder; and Mike Tate, Chief Financial Officer. Before we get started, I\u2019d like to remind everyone that certain comments made in this call today may include forward-looking statements regarding, among other things, expected future financial results, strategies and plans, future operations and the markets in which we operate. These forward-looking statements reflect management's current beliefs, expectations and assumptions about future events which are inherently subject to risks and uncertainties that are discussed in detail in today's earnings release and the periodic reports and filings that we file from time to time with the SEC, including the risks set forth in the final prospectus relating to our IPO. It is not possible for the company's management to predict all risks and uncertainties that could have an impact on these forward-looking statements or the extent to which any factor or combination of factors may cause actual results to differ materially from those contained in any forward-looking statements. In light of these risks, uncertainties and assumptions, the results, events or circumstances reflected in the forward-looking statements discussed during this call may not occur, and actual results could differ materially from those anticipated or implied. All of our statements are based on information available to management as of today, and the company undertakes no obligation to update such statements after the date of this call to conform to these as a result of new information, future events or changes in our expectations, except as required by law. Also during this call, we will refer to certain non-GAAP financial measures, which we consider to be an important measure of the company's performance. These non-GAAP financial measures are provided in addition to and not as a substitute for or superior to financial results prepared in accordance with US GAAP. A discussion of why we use non-GAAP financial measures and reconciliations between our GAAP and non-GAAP financial measures is available in the earnings release we issued today, which can be accessed through the Investor Relations portion of our website and also be included in our filings with the SEC, which will also be accessible through the Investor Relations portion of our website. With that, I\u2019d like to turn the call over to Jitendra Mohan, CEO of Astera Labs. Jitendra?\nJitendra Mohan: Thank you, Leslie. Good afternoon, everyone, and thanks for joining our second quarter conference call for fiscal 2024. AI continues to drive a strong investment cycle, as entire industries look to expand their creative output and overall productivity. The velocity and dynamic nature of this investment in AI infrastructure, is generating highly complex and diverse challenges for our customers. Astera Lab's intelligent and flexible connectivity solutions are developed ground up to navigate these fast-paced, complicated deployments. We are working closely with our hyperscaler customers to help them solve these challenges across diverse AI platform architectures that features both third-party and internally developed accelerators. In addition to these favorable secular trends, we are also benefiting from new company specific product cycles across multiple technologies, which will also contribute to our growth in the form of higher average silicon content per AI platform. A strong leadership position and great execution by our team resulted in record revenue for Astera Labs in the June quarter supports our strong outlook for the third quarter and gives us confidence in our ability to continue outperforming industry growth rates. Astera Labs delivered strong Q2 results, setting our first consecutive record for quarterly revenue, strong non-GAAP operating margin and positive operating cash flows. Our revenue in Q2 was $76.9 million up 18% from the previous quarter and up 619% from the same period in 2023. Non-GAAP operating margin was 24.4%, and we delivered $0.13 of non-GAAP diluted earnings per share. Operating cash flow generation was also strong during the quarter, coming in at $29.8 million. With continued business momentum and a broadening set of growth opportunities, we are investing in our customers by rapidly scaling the organization. During the quarter, we expanded our Cloud-Scale Interop Lab to Taiwan and announced the opening of a new R&D center in India. We also announced the appointment of Bethany Mayer to our Board of Directors, bringing additional strategic leadership to the company. Today, Astera Labs is focused on three core technology standards; PCI Express, Ethernet, and Compute Express Link. We are shipping three separate product families supporting these different connectivity protocols, all generating revenue and in various stages of adoption. Let me touch upon our business with each of these product families and how we support them with our differentiated architecture and COSMOS software suite. Then I will turn the call over to Sanjay to dive deeper into our growth strategy. Finally, Mike will provide additional details on our Q2 results and our Q3 financial guidance. First, let us talk about PCI Express. During the quarter, we saw continued strong demand for our Aries product family to drive reliable PCI Gen 5 connectivity in AI systems by delivering robust signal integrity and link stability. While merchant GPU suppliers drove early adoption of PCI Gen 5 into real systems over the past year, we are now also seeing our hyperscaler customers introduce and ramp new AI server programs based upon their internally developed accelerators utilizing PCI Gen 5. Looking ahead, AI accelerator processing power is continuing to increase at an incredible pace. The next milestone for the AI technology evolution is the commercialization of PCI Gen 6, which doubles the connectivity bandwidth within AI servers, creating new challenges for link reach, reliability and latency. Our Aries 6 PCI Retimers family helps to solve these challenges with the next generation of our software-defined architecture, offering a seamless upgrade path to a widely deployed and field-tested Gen 5 solutions. We have started shipping initial quantities of preproduction orders of our PCIe Gen 6 solution, Aries 6. We ship and support our hyperscaler customers initial program developments that are based on Nvidia's Blackwell platform, including GB200. We look forward to supporting more significant production ramps in the quarters to come. Next let us talk about Ethernet. Our portfolio of Taurus Ethernet smart cable modules helps relieve connectivity bottlenecks by overcoming reach, signal integrity and bandwidth issues by enabling robust 100-gig per lane connectivity over copper cables or AEC. Today, we are pleased to announce that our 400-gig Taurus Ethernet SCMs have shifted into volume production, with an expected ramp through the back half of 2024. This ramp is happening across multiple platforms in multiple cable configurations, and we are working with multiple cable partners to support the expected volumes. Taurus will be ramping across a multitude of 400-gig applications to scale out connectivity on both AI compute platforms, as well as general purpose compute systems. We are excited about the breadth and diversity of our Taurus design wins and expect the product family to be accretive to our corporate growth rate going forward. Next is Compute Express Link, or CXL. We continue to work closely with our hyperscaler customers on a variety of use cases and applications for CXL. In Q2, we shipped material volume of our Leo products for preproduction large-scale deployment in data centers. We expect to see data center platform architects utilize CXL technology to solve memory bandwidth and capacity bottlenecks using our Leo family of products. The initial deployments are targeting memory expansion use cases, with production ramps starting in 2025 when new CXL-capable CPUs are broadly available. Finally, I\u2019d like to spend a moment on COSMOS, which is a software platform that brings all of our product families together. We have discussed how COSMOS not only runs on our chips, but also in our customers' operating stacks to deliver seamless customization, optimization and monitoring. The combination of our semiconductor and hardware solutions with COSMOS software enables our product to become the eyes and ears of connectivity infrastructure, helping fleet managers to ensure their AI and cloud infrastructure is operating at peak utilization. By improving the efficiency of their data centers, our customers are able to generate higher ROI and reduce downtime. To summarize sustained secular trends in AI adoption, design wins across diverse AI platforms at hyperscalers, featuring both third-party and internally developed accelerators an increasing average dollar content in next-generation GPU-based AI platforms gives us confidence in our ability to outperform industry growth rates. With that, let me turn the call over to our President and COO, Sanjay Gajendra to discuss some of our recent product announcements and our long-term growth strategy.\nSanjay Gajendra: Thanks, Jitendra and good afternoon, everyone. We are pleased with our robust Q2 results and strong top-line outlook for Q3. But we are even more excited about the volume and breadth of opportunities that lie ahead. Today, I will focus on five growth vectors that we believe will help us to grow our business faster than industry growth rates over the long term. First, Astera Labs is in a unique position with design wins across diverse AI platform architectures featuring both third-party and internally developed accelerators. This diversity gives us multiple paths to grow our business. This hybrid approach of using third-party and internally developed accelerators allows hyperscalers to optimize their fleet to support unique workload requirements and infrastructure limitations, while also improving capital investment efficiency. Our intelligent connectivity platform with its flexible software-based architecture enables portability and seamless reuse between platforms while creating growth opportunities for all our product families. In addition to the third-party GPU platforms, we also expect to see several large deployments based on internally developed AI accelerators hitting production volume over the next few quarters and driving incremental PCIe and Ethernet volumes for us. Second, we see increasing content on next-generation AI platforms. Nvidia's Blackwell GPU architecture is particularly exciting for us, as we expect to see strong growth opportunities based on our design wins as hyperscalers compose solutions based on Blackwell GPUs, including GB200 across their data center infrastructure. To support various AI workloads, infrastructure challenges, software, power and cooling requirements, we expect multiple deployment variants for this new GPU platform. For example, Nvidia cited 100 different configurations for Blackwell in their most recent earnings call. This growing trend of complexity and diversity presents an exciting opportunity for Astera Labs as our flexible silicon architecture and COSMOS software suite can be harnessed to customize the connectivity backbone for a diverse set of deployment scenarios. Overall, we expect our business to benefit from the Blackwell introduction with higher average dollar content of our products per GPU, driven by a combination of increasing volumes and higher ASPs. The next growth vector is the broadening applications and use cases for our Aries product family. Aries is in its third generation now and represents the gold standard for PCIe Retimers in the industry. The introduction of the new Aries 6 Retimers built upon the company's widely deployed and battle-tested PCIe 5 retimers and the industry transition to PCIe Gen 6 will be a catalyst for increasing PCIe retimer content for Astera. Our learnings from hundreds of design wins and production deployment over the last several years enables us to quickly deploy PCIe Gen 6 technology at scale. As Jitendra noted, we\u2019re now shipping initial quantities of preproduction volume for Aries 6 and currently have meaningful backlog in place to support the initial deployment of hyperscaler AI servers featuring Nvidia's Blackwell GPUs including GB200. We're also very excited about the incremental PCIe connectivity market expansion that will be driven by multi-rack GPU clustering. Similar to the dynamic within the Ethernet AEC business, the reach limitations of passive PCIe copper cables are a bottleneck for the number of GPUs that can be clustered together. Our purpose-built Aries smart cable modules solve these issues by providing robust signal-integrity and link stability over materially longer distances improving rack airflow, while actively monitoring and optimizing link health. This PCIe AEC opportunity is in the early stages of adoption and deployment and we view the multi-rack GPU clustering application as a new and growing market opportunity for our Aries product family. In June, we announced the industry's first demonstration of end-to-end PCIe optical connectivity to provide unprecedented reach for larger GPU clusters. We are proud to broaden our PCIe leadership once again by demonstrating robust PCIe links over optical interconnects between GPUs, CPUs, CXL memory devices and other PCIe endpoints. This breakthrough expands our intelligent connectivity platform to allow customers to seamlessly scale and extend high-bandwidth, low-latency PCI interconnects over optics. Overall, we expect our Aries PCIe retimer business to deliver strong growth as system complexity, platform diversity and speeds continue to increase and on average, result in higher retimer content per GPU in next-generation AI platforms. Next in addition to the strong growth prospect of our Aries product family across the PCIe ecosystem, we are also seeing our Taurus product family for Ethernet AEC application start to meaningfully contribute to the growth in the back half of 2024. What is exciting about these ramps is the diversity in application and use cases. We are seeing demand for our Taurus product family for both AI and general compute platforms. We\u2019re supporting the market with multiple cable configurations, including straight, Y cables and X cables. We will be shipping volume into hyperscaler build-outs, supporting multiple cable vendors to enable a diverse supply chain that is crucial for hyperscalers. Overall, we are very excited about Taurus becoming yet another engine of growth as we look to expand the top-line while also diversifying our product family contributions. Last but not least, CXL is an important technology to solve memory bandwidth and capacity bottlenecks in compute platforms. We are working closely with our hyperscaler partners to demonstrate various use cases for this technology and starting to deploy our Leo CXL controllers in preproduction racks in data centers. We have incorporated the learnings, customization and security requirements into our COSMOS software and have the most robust, cloud-ready CXL solution in the industry. We have demonstrated that our Leo CXL Smart Memory Controllers improve application performance and reduce TCO in compute-platforms. Very importantly, we can accomplish many of these performance gains with zero application-level software changes or upgrades. Overall, we remain very excited about the potential of CXL in data center applications. Finally, our close collaboration and front-row seat with hyperscalers, and AI platform providers continues to yield valuable insights regarding the direction of compute technologies and the connectivity topologies that will be required to support them. This close collaboration is helping us identify new product and business opportunities and additional engagement models across our entire intelligent connectivity platform, which we believe will drive strong, long-term growth for Astera. With that, I\u2019ll turn the call over to our CFO Mike Tate, who will discuss our Q2 financial results and our Q3 outlook.\nMike Tate: Thanks, Sanjay, and thanks to everyone for joining the call. This overview of our Q2 financial results and Q3 guidance will be on a non-GAAP basis. The primary difference in Astera Labs' non-GAAP metrics is stock-based compensation and its related income tax effects. Please refer to today's press release available on the Investor Relations section of our website for more details on both our GAAP and non-GAAP Q3 financial outlook, as well as a reconciliation of our GAAP to non-GAAP financial measures presented on this call. For Q2 of 2024, Astera Labs delivered record quarterly revenue of $76.9 million, which was up 18% from the previous quarter and 619% higher than the revenue in Q2 of 2023. During the quarter, we shipped products to all major hyperscalers and AI accelerator manufacturers. We recognized revenue across all three of our product families during the quarter, with the Aries product being the largest contributor. That is seen from continued momentum in AI based platforms. In Q2, Taurus revenues contributed to -- continued to primarily shift into 200-gig Ethernet-based systems, and we expect Taurus revenue to now diversify further as we begin to ship volume into 400-gig Ethernet-based systems in the third quarter. Q2 Leo revenues were largely from customers purchasing preproduction volumes for the development of their next-generation, CXL capable compute platforms, with our customers' production launch timing being dependent on the data center server CPU refresh cycle. Q2 non-GAAP gross margins was 78% and was down 20 basis points compared to 78.2% in Q1 of 2024 and better than our guidance of 77%. Non-GAAP operating expenses for Q2 were $41.2 million, up from $35.2 million in the previous quarter and consistent with our guidance. Within non-GAAP operating expenses, R&D expenses was $27.1 million; sales and marketing expense was $6.3 million and general and administrative expenses was $7.8 million. Non-GAAP operating margin for Q2 was 24.4%. Interest income in Q2 was $10.3 million. Our non-GAAP tax provision was $6.8 million for the quarter, which represents a tax rate of 23% on a non-GAAP basis. Non-GAAP fully diluted share count for Q2 was 175.3 million shares and our non-GAAP diluted earnings per share for the quarter was $0.13. Cash flow from operating activities for Q2 was $29.8 million, and we ended the quarter with cash, cash equivalents and marketable securities of just over $830 million. Now turning to our guidance for Q3 of fiscal 2024. We expect Q3 revenue to increase within a range of $95 million and $100 million, up roughly 24% to 30% sequentially from the prior quarter. We believe our Aries product family will continue to be the largest component of revenue and will be the primary driver of sequential growth in Q3, driven by growing volume deployment with our customers' AI servers. We also expect our Taurus family to drive solid growth quarter-over-quarter as design wins within new 400-gig Ethernet-based systems ramp into volume production. We expect non-GAAP margins to be approximately 75%. The sequential decline in gross margin is being driven by an expected product mix shift towards hardware solutions during the quarter. We expect non-GAAP operating expenses to be in the range of approximately $46 million to $47 million as we remain aggressive in expanding our R&D resource pool across head count and intellectual property. Interest income is expected to be approximately $10 million. Our non-GAAP tax rate should be approximately 20%, and our non-GAAP fully diluted share count is expected to be approximately 177 million shares. Adding this all up, we\u2019re expecting non-GAAP fully diluted earnings per share of a range of approximately $0.16 to $0.17. This concludes the prepared remarks. And once again, we are very much appreciative of everyone joining the call. And now we will open the line for questions. Operator?\nOperator: Thank you. [Operator Instructions] We'll take our first question from Harlan Sur at JPMorgan.\nHarlan Sur: Good afternoon. Thanks for taking my question and congratulations on the strong results. During the quarter, lots of concerns around your large GPU customer and one of their next-generation GPU SKUs, the GB200. Glad that the team could clarify that your dollar content across all Blackwell GPU SKUs is actually rising versus prior-generation Hopper. But as you guys mentioned, AI ASIC accelerator mix is rapidly rising, and actually we believe outgrowing GPUs both this year and next year and accounting for like 50% of the XPU mix sort of next year, right? And with ASIC, it's 100% PCIe based. And as you pointed out, right, many of these ASIC customers are still in the early stages of the ramp. So given all of this, given some of the new product ramps with your AEC solution. What's the team's visibility and confidence level on driving continued quarter-on-quarter growth from here maybe over the next several quarters?\nJitendra Mohan: Harlan, thank you so much for the question. It is great to be in the place that we are here today. We feel very confident about what is to come. Clearly we don't guide more than one quarter out, so please don't take that as any guidance. But we really believe that we are in the early innings of AI here. All of the hyperscalers are increasing their CapEx targets for the rest of the year. 2025 is expected to be even higher. We heard that the Llama model requires 10 times more compute in order to solve that. So all of these trends are basically driving a radical shift in technology. We are seeing, as you correctly pointed out, a lot of our hyperscaler customers ramp their internally developed AI accelerators in addition to deploying third-party AI accelerators. And we are very pleased that we have designed wins across all of these different platforms. Our customers are ramping their platforms and we are ramping multiple product families. As Sanjay mentioned, both Aries and Taurus are ramping into these new platforms. So we feel very good about what is in store for the future and feel that with the rising content on a per GPU basis, we\u2019ll be able to outpace the market growth in the long-term.\nHarlan Sur: I appreciate that. And on top of the strong AI demand trend pulls, on top of the new product ramps that you guys articulated today, one thing I haven't baked in to my model is the penetration of your retimer technology into general purpose servers, right? And the good news is that we are finally starting to see the flash vendors aggressively bringing, finally bringing Gen 5 PCIe SSDs to the market, which could potentially unlock the retimer opportunities in general purpose servers where the Gen 5 retimer content today is still 0. So what's the team's outlook? Do you see that there may be some penetration starting in 2025 of your retimer solutions into general purpose server? And maybe if you could size that potential opportunity for us.\nSanjay Gajendra: Absolutely, Harlan. Sanjay here. Good to hear your voice. Yes. So in general, that's a correct statement. We have several design wins on the compute side. Just for reasons like you highlighted, either SSDs not being Gen 5-ready or for dollars being sort of factoring into the AI platforms. There has been a slower than expected growth on the general compute. But at some point, like we keep saying, the servers are going to fall off the rack at some point given how long they've been in the fleet. So we do expect the general compute to start picking up, especially as both AMD and Intel get to production with the Turin and the kind of Granite Rapids based CPUs. So overall 2025, we do expect that the compute platform will start figuring in terms of being meaningful revenue growth. Like I noted, we do have design wins already in these platforms for Aries retimers. But also I would like to add that we do have design wins for our Taurus Ethernet module application as well in general compute. So we should see sort of the two-engine growth story along the general compute to go along with all of the things we shared on AI, both for third-party or merchant GPUs, as well as the big change that we are seeing now is the ramp in the internally developed accelerators, those things being a meaningful and a significant driver for our growth.\nHarlan Sur: Thank you.\nOperator: We'll move next to Joe Moore at Morgan Stanley.\nJoe Moore: Great. Thank you. I wonder if you could talk about the competitive dynamics within PCI Gen 5 retimers. Are you seeing any number of people who have qualified solutions in China and in the US? Are you seeing any encroachment there? And then in terms of PCI Gen 6, can you talk about the prospects for when you start to see volume there?\nSanjay Gajendra: Yes, absolutely. Let me take that, Joe. So overall, this is a big and growing market. I think that fact is clear. I mean the fact that you have larger names jumping into the mix sort of validates the market that the retimer represents. Now a couple of points to keep in mind is that connectivity products, especially PCI Express, tends to have a certain nuance to it which is the fact that we are the device in the middle. We are always in between GPU, storage, networking and so on. And interoperation, especially at high volume, cloud-scale deployment becomes critical. So what we have done in the last three years, four years is really work shoulder-to-shoulder with our hyperscaler and AI platform providers to ensure that the interoperation is met, the platform-level deployment, whether it is diagnostic, telemetry, firmware management is all addressed, including the COSMOS software that we provide from a management -- fleet management and diagnostic type of capability. Those all have been integrated into our customers' operating stack. So in general, the picture I'm trying to paint here is that the tribal knowledge that we have built, the penetration that we have not just with the silicon, but also software does give us a significant advantage compared to our competitors. Now having said that, we will continue to work hard. We have several design wins for PCIe Gen 6 like we shared in today's call that are all designed around the next-generation GPU platform, specifically the Blackwell-based GPUs from Nvidia, which I publicly noted to support Gen 6. So we'll continue to work through them. We are currently shipping preproduction volume to support some of the initial ramps, including for GB200 based platforms. So overall, we feel good about the position that we are in, both in terms of Gen 5, as well as transitioning those designs into Gen 6 as the platforms develop and grow.\nJoe Moore: Great. Thank you.\nOperator: We'll move next to Blayne Curtis at Jefferies LLC.\nBlayne Curtis: Hi, thanks for taking my question. I just want to ask you in terms of the September outlook, you talked about meaningful revenue from AEC. I mean I think the other point was that the gross margin was because of the mix, which I'm assuming is because of that ramp. But just trying to size it, I know you don't break out the segments. But if you can kind of just give us some broad strokes as to how much of the growth is coming from retimers versus Aries in September.\nMike Tate: Yes, Blayne. The margins will come down to the extent we sell more hardware versus silicon. So Taurus is definitely one of those drivers. Also we do modules on the Aries side, and both -- we're seeing growth in both of those. So when you look at the growth guidance we are giving in third quarter, you have the contribution from Taurus, you have the incremental modules on Aries, but also we are seeing a lot of growth just from Aries Gen 5 going into AI servers. And a lot of new platforms, and the platforms generally are getting more content per platform. So when you look at the growth, I think it is kind of balanced between those three drivers, largely.\nBlayne Curtis: Got you. And then I want to ask on the Gen 6 adoption moving from preproduction to production. The main GPU in the market supports Gen 6. I think the CPUs that would talk Gen 6 are going to be a bit of a way -- over your way. So I'm just kind of curious, the catalyst there, do you expect Gen 6 to be in the market next or even if there is not CPUs that kind of speak Gen 6?\nJitendra Mohan: Blayne, it is a great observation. Let me say that as these compute platform gets more and more powerful to address these growing AI models, the only way to keep them fed, to keep these GPUs utilized is to get more and more data in and out of these platforms. So in the past, the CPUs played a very central role in terms of being the can't do it for all of this information. But with the new accelerated compute architecture, CPU is largely orchestration of a control engine for the most part, it does do a few other things. But in general, you are trying to get the data in and out of the GPU, using the scale out and scale up networks that are made up of either PCI Express, Ethernet or NVLink protocols. And as these protocols go faster and faster, we end up seeing more and more demand for the product that we have. And as a result, as these new systems get deployed, we see higher content for us on a per GPU basis. and it's largely to improve the GPU utilization through these increased data rates.\nSanjay Gajendra: Blayne, if I can add one more point. You didn't quite directly asked us for the September quarter growth. I do want to be abundantly clear on one point, which is the growth that we are forecasting for September quarter is based upon not just the power sampling, but all of the additional production ramps that we are seeing for both the third-party platforms, but also internally developed accelerators. That is what is modeling and driving the growth that we are highlighting for September although there are maybe other things that you can look at the overall stuff.\nOperator: We'll move to our next question from Tom O'Malley at Barclays.\nThomas O\u2019Malley: Hi, guys. Thanks for taking my questions. Congrats on a nice results. I just wanted to ask a broader network architecture question. You talked a little bit more about PCIe over optical. And when you look at the back end today, I think there's a lot of efforts to improve the Ethernet offering as it compares to kind of the leader in the market as they kind of expand NVLink. Could you talk about when you see the inflection point with PCIe over optical kind of being the majority of the back end? Is that something that's coming sooner? Just kind of the time frame there. And then just explain a little further, I think you mentioned that it comes with a lot of additional retiming content when you use those cables. Just anything additional there, and then I have a follow-up.\nJitendra Mohan: Let me take that. This is Jitendra, Tom. The architectures for AI systems are definitely evolving. And actually, I would say, they are evolving at a very rapid pace. Different customers use different architectures to craft their systems. If you look at Nvidia-based systems, they do use NVLink, which is, of course, a proprietary closed interface. The rest of the world largely uses protocols that are either PCI Express or Ethernet or they are based on PCI Express and Ethernet. And the choice of particular protocol is really dependent upon the infrastructure that the hyperscalers have and how they choose to deploy this technology. Clearly, we play in both. Our Taurus Ethernet Smart Cable Module support Ethernet. And now with our Aries Smart Cable Modules, we are able to support our PCI Express as well. And if you think about the evolution, we started with Aries retimers for driving mostly within the box connectivity and shorter distance connectivity over passive cables. As these networking architecture evolved and you needed to cluster more GPUs together, we went with the Aries smart cable modules that allow you to connect multiple racks together, up to 7 meters of copper cables. And as it expands into even further distances, we go into optical where we demonstrated running a very popular GPU over 50 meters of optical fiber. So these are all of the tools that we are making available to our hyperscaler partners for them to craft their solutions and deploy AI at the data center scale.\nThomas O\u2019Malley: Helpful. As a follow-up, I know this is a bit of a tougher question, but I do think that there is a lot of confusion out there. And I just would appreciate your thoughts. You mentioned in the prepared remarks hundreds of different types of deployment styles for the GB200. Obviously, certain hyperscalers are going to do it their way and then certain hyperscalers are going to take what's called the kind of entire system, so the $36 million of the $72 million. Can you talk about your assumptions for what you think will be the percentage that goes towards the full system and then kind of towards the hyperscalers that use kind of their own methods and talk about the content opportunities if they would kind of play out in those two scenarios? I do think that Nvidia and others are talking potentially about more systems than historical, but just maybe the puts and takes upon how different hyperscalers or architect systems and what it means for your content. Thank you.\nJitendra Mohan: Yes. Great questions. And as you pointed out, a lot of moving pieces obviously, right? But here is, I think, what we know and what we can comment on. First of all, all the hyperscalers are indeed deploying new AI platforms that are based on merchant silicon or third-party accelerators, as well as their own accelerators. And overall, we do expect our retimer content to go up. Now if you double-click specifically on Nvidia or the Blackwell system, it comes in many, many different flavors. If you think about the overall Blackwell platform, it is really pushing the technology boundaries. And what that is doing is it's creating more challenges for the hyperscalers, whether it is power delivery or thermals, software complexities or connectivity. As these systems grow bigger, they run faster, become more complex, we absolutely think that the need for retimer goes up. And that drives our content higher on a per GPU basis. Now it is harder to predict which particular platform will have what kind of share. That's not really our business to predict. What we are doing is we are supporting our customers, our AI platform providers as well as hyperscalers to make sure that these kind of high-tech platforms can be deployed as easily as possible. And at the end of the day, what you will find is hyperscalers will have to either adopt their data centers to these new technologies or they'll have to adopt this new technology to their data centers. And that creates a great opportunity for our products. We already have design wins across multiple form factors of hyperscaler GPUs as well as the third-party GPUs. And overall, we expect our business to continue to grow strongly. Very exciting times for us.\nThomas O'Malley: Thank you very much.\nOperator: Our next question comes from Tore Svanberg at Stifel, Nicolaus.\nJeremy Kwan: Yes. Good afternoon. This is Jeremy calling for Tore. And let me also add my congratulations on a very strong quarter and outlook. A couple of questions. First, could you provide maybe a revenue breakout between the three product segments here? I'm not sure if that was covered at all.\nMike Tate: Yes. We don't break out specifically the revenue by product. But like we said on the call, the Q2 revenues were driven heavily by the AI growth for Gen 5 and the broadening out of our design win portfolio. When you look into Q3, it's -- the three main drivers are the initial Taurus ramp into 400 gig, the broadening out of AI servers for Gen 5 in both merchant as well as internally developed accelerator programs, and then also we are doing back-end clustering with our Aries SCM modules. So when you look at that, those three drivers are mainly giving us the growth in Q3.\nJeremy Kwan: Great. And then I guess maybe looking more into the Leo CXL. I understand you are shipping preproduction. Is the -- when can -- are you expecting to see more of a material ramp for Leo?\nSanjay Gajendra: Yes. So in terms of material ramp, it's a function of CPUs being available that supports CXL 2.0. So we are of course, tracking the announcements from AMD and Intel to essentially get to production in the second half of this year for Turin and Granite Rapids. So in general, these things will take a little time in terms of engineering those things into platforms. So what we are guiding is 2025 is when we expect production ramps to pick up on CXL.\nJeremy Kwan: Great. Thanks. And if I could squeeze one last question in. Can you give us maybe a sense of your revenue, how it might break out between modules and stand-alone retimers? Is there a way to kind of look at revenues in that way and how that can impact your SAM growth over time? Thank you.\nMike Tate: Yes. Taurus predominantly is modules. Aries, we are doing the back-end clustering of GPUs with the modules predominantly, but the bulk of the revenues is stand-alone retimers in that product family. Leo, once it ramps, we\u2019ll do add-in cards and silicon, but they\u2019ll be heavily skewed towards silicon.\nOperator: We'll move next to Quinn Bolton at Needham & Company.\nQuinn Bolton: Hi, guys. Thanks for taking my question. I guess maybe a follow-up just on the Blackwell question. It looks like there have been some recent architectural or system-level changes at Nvidia with sort of the introduction of the GB200A that looks like it uses PCI interconnect or PCI Express to connect the GPUs and the CPUs and perhaps a deemphasis in the HGX platforms. Just wondering if you see any shifts in content, if that's favorable, if it's about a wash going from one platform to the other. And then I've got a follow-up.\nJitendra Mohan: Yes. Thank you, Quinn. Unfortunately, it will not be appropriate for us to kind of comment on rumors and third-party information that seems to be circulating around. What we will say is that we are committed to whatever platform our customers want to deploy. Whether it's a full rack or it's an HGX server or something in between, we are working with them very, very closely, shoulder to shoulder every day. As Sanjay mentioned, we already have multiple design wins in the Blackwell family including the GB200. We are shipping initial quantities of preproduction to the early adopters. And we do have backlog in place that serves the Blackwell platform, including GB200.\nQuinn Bolton: Got it. Okay. Thank you for that. And just maybe a clarification on the Taurus 400-gig ramp as well as the Aries SCM ramps. Are those ramping across multiple hyperscalers? Are they driven by a lead hyperscaler initially and then you would expect to broaden it out to other hyperscalers as we move into 2025?\nSanjay Gajendra: Yes. Good question. Let me take that. So if you think about AECs, in general, 800 gig, where you're running 100 gig per lane is the first broad use case that we believe for AEC applications. If you look at [data] (ph) rates lower than that, let's say, 400 gig and so on, it tends to be very, frankly, case by case. It depends on the topology, application and so on. So the good thing about the design wins we have is that these scale across multiple platforms both from an AI and general compute standpoint and supporting various different topologies. And the revenue drivers that we are essentially highlighting for 3Q and beyond is based on supporting these applications. With 800 gig, it becomes much more broader with several different customers essentially requiring AECs.\nQuinn Bolton: And is it similar for the Aries SCMs for back-end clustering as well?\nSanjay Gajendra: Exactly. It depends on the topology for what it is in terms of how the back-end networks are designed for the AI subsystems. In general all of this -- when it comes to active cabling type of technology, it becomes case by case depending on the infrastructure and how exactly systems are being put together compared to a component like a PCIe retimer, that goes across a broad array of use cases across multiple different deployment scenarios. So that's the nuance to keep in mind when you look at AEC markets. And so the volume and the deployment scale tends to be very broad, right, if you are looking at how infrastructures are being put together. So it is one of those things where you look at case by case. But as long as you're able to address a wide variety of applications, it does very significantly add up.\nOperator: We'll take our next question from Ross Seymore at Deutsche Bank.\nRoss Seymore: Hi, guys. Thanks for asking the question. Apologies if I go back to one that's been hit on a couple of me, so I want to do it nonetheless, and kind of the Blackwell topic and the content topic. You guys gave us the punchline that you believe your content, on average, will go up per GPU generation to generation. It also seems like you're getting across that the customization of it is still very broad-based. And so just looking at the vanilla system SKUs and reference design Nvidia itself has might be misleading. Two-part question to this. Are you of the belief that your content is equal across the board in the same way it was in Hopper? Or do things get more skewed where there'll be places where you'll have a significant step-up in content and some configurations and others where you would have a significant step down? And the difference between those two might be where investors are getting a little bit confused.\nSanjay Gajendra: Let me try to add a little bit more color on that. But before I do that, let me give you and remind two data points we've already covered in the Q&A so far. First point, let us be very clear that our PCIe retimer content per GPU, on average will continue to grow as the AI systems scales across various different topologies. And this applies to both third-party, like standard merchant GPUs as well as internally developed GPUs. The second reminder that I want to kind of note is that specifically for Blackwell, we expect our PCIe content per GPU to go up. Now what you are asking is specifically about the deployment scenarios, which right now is evolving, right? So we have design wins for several different topologies, including the GB200. But if you look at the various different options that Nvidia is offering and how those are being composed and considered by the hyperscalers, that situation is evolving at the moment. The key message that we want to deliver is that, overall, our PCIe content is going to be higher than the Hopper generation. We expect that the design wins that we are starting to see and we're starting to ship from a pre-production standpoint are all meaningful that will essentially allow us to continue to have a robust growth engine, as far as our PCIe retimer business is concerned.\nRoss Seymore: Thanks for that. And I guess as a follow-up, you guys have focused more on this call about the internally developed accelerators than you have in calls in the past. And I realize there haven't been too many since your IPO. But are you trying to get across the key message that those are really growing as a percentage of your mix, that those are penetrating the market and kind of catching up and taking relative share from the GPU side of things? Or is your commentary meant to get across that Astera itself with its retimers and other components will take significant share in that kind of ASIC market relative to the GPU side?\nSanjay Gajendra: Yes, it's probably both, to be honest with you, in the sense that we do see it, it's no secret, right? I think many of the hyperscalers are doing their own accelerators, which are driven by the workloads or the business models that they pursue. I think that will continue as a macro trend in terms of internally developed accelerators going hand in hand with GPUs that are available from Nvidia or AMD or others. So that's the model that we believe will be here to stay, that hybrid approach. And for us really, the reason we are highlighting is that, of course, we have had a significant business that has grown in the last year or two years from the designs that we have been supporting with the merchant GPU deployments that have happened. But at the same time, now we're reaching a point where the accelerator volumes are also starting to ramp up. And for us the good news is that we are on all the major AI accelerator platforms from a design win standpoint or at least all the major ones that are out there. And for us, we have multiple parts to grow our business, and that is a very positive thing that we believe will continue to allow us to keep delivering the kind of reserves that we're doing. And as new CPU/GPU architectures come about, just like the Nvidia's Blackwell platform, we do expect to gain from it both on the retimer content as well as other products that we can service to this space.\nOperator: We'll take our next question from Richard Shannon at Craig-Hallum Capital Group.\nRichard Shannon: Hi, guys. Thanks for taking my question. Maybe a question on PCI Express Gen 6 here. Last call, you talked about some of the wins, designs being decided in the next six months to nine months are obviously three to six months -- three more months farther forward here. Obviously you've got some wins already on Gen 6, but I just want to get a sense of the share of the market kind of looking backwards. How much of that market has been decided versus up for grabs? Maybe you can help characterize what's left here to win in the next three months to six months.\nSanjay Gajendra: I'm trying to see how best to answer that question. So you got to -- let me try to provide some color. The design win windows is, whatever, for these platforms, it's you're looking at -- once GPUs become available, you're looking at 6 to 12 months before they go to production. So that's one thing to keep in mind. But also please also think about how hyperscalers go about doing their stuff, right? Everyone is in an arms' race right now getting to production as quickly as possible. In many different situations resources are also limited. Meaning for every 10 engineers that they may need, they might have two or three, just given the number of platforms and how quickly everyone is trying to move. And to that standpoint, what is happening is that many of these engineers are familiar with our Gen 5 retimers. They've designed it across multiple platforms. They've built software tools and capabilities around it. And now our Gen 6 retimers are essentially a seamless upgrade from a software standpoint, from a hardware standpoint. So it does offer the lowest risk and fastest path to our customers. And that plays well within their own objectives of trying to get something out quickly and dealing with resources that might not be available at the levels that are required. So overall we\u2019re starting to gain from it, and we are essentially being the leader in the space, being the one that is getting the first crack at these opportunities. And we are doing everything we can to convert those things into design wins and revenue.\nRichard Shannon: Okay. Great. My follow-on question is a pretty simple one, just looking at the Taurus line here. Great to see the ramp, you're at 400 gig, and I don't want to kind of get too far ahead of what looks to be a pretty nice ramp here in the second half of the year, but I think you've talked about the 800-gig generation ramping later in 2025. Any update on that timing and how your design win is looking so far?\nJitendra Mohan: Yes. Good question. So the 800-gig timing, we believe, is going to be late in 2025. Right now, what we are seeing is 400-gig applications for some of the AI systems as well as actually we are seeing them for general purpose compute as well where you are doing the traditional server to top of the rack connection. So that will continue on for the rest of this year for 400 gig deployments. And then as we get some of the newer mix that are capable of 100 gig and 200 gig per lane, et cetera are try to get to 800 gig is where we see broadening of this market and more deployments across different hyperscalers across different platforms in the latter half of 2025.\nRichard Shannon : Okay. Great. Thanks guys.\nOperator: We'll go next to Suji Desilva at ROTH Capital.\nUnidentified Analyst: Hi, Jitendra, this is [Andre] (ph). And Mike, congrats on the progress here. This question maybe may not have been asked explicitly, but can you give us a relative content framework for internally developed versus third-party processors or accelerators? Is it higher for internally developed on average? Or is it hard to generalize like that?\nJitendra Mohan: I would say it's a little bit hard to generalize. It varies quite a bit. Even one particular platform, you can have different form factors. Even if you look at, let's say, Blackwell, you have HGX, you have MGX, you have NVLs, you have custom racks that are getting deployed. And if you look at each one of them, you'll find different amount of content. Number of retimers will vary where they get placed. It will vary -- but what is very consistent is that the overall content does go up for us. Now the other factor to consider is the choice of back-end network. Again, for example if you look at the Blackwell family, they use NVLink, which is a closed proprietary standard, which we do not participate in. But when somebody uses a PCI Express or PCI Express-based protocol at their back-end connectivity, then our content goes up pretty significantly because now we are shipping not only our retimers but also the smart cable -- Aries smart cable modules into that application. Similarly, if the back-end interconnected Ethernet, that will benefit our Taurus family of product lines. So it really varies greatly on what the architecture is of the platform and what form factor is getting deployed in.\nUnidentified Analyst: Okay. Great. That's very helpful color. Thanks. And then just a quick follow-up here. Was there something inherent in the Blackwell transition from Hopper that made this much platform diversification and our ex-diversification possible? Or was it just the hyperscalers getting more sophisticated about what they are trying to do? Or was it availability of things like Astera's PCI products? Any color there would be helpful as to how this kind of proliferation of architectures kind of came about.\nJitendra Mohan: I mean if you look at the Blackwell family, it's like a marvel of technology. The amount of content that is being pushed into that platform is incredible. And as I mentioned earlier, that does create other problems, right? There is so much compute packed in such small space. They're delivering power to those racks -- to those GPUs themselves and the CPUs is the challenge. How to cool them, it becomes a challenge. And the fact that modern data centers are just not equipped to handle many of these issues. So what the hyperscalers are doing is they're taking these broad platforms in our technology and trying to adapt it so that it fits into their data centers. And that's where we see a lot of opportunity for our existing products, the ones that we have talked about, as well as some new products that we've been working on again, shoulder to shoulder with our hyperscaler and air platform customers. So very excited to see how these new platforms will get rolled out, including Blackwell, including the hyperscaler internal AI platforms and the increased content that we have there.\nUnidentified Analyst: Okay. Great. Thanks for the color.\nOperator: And finally, we'll move to Quinn Bolton at Needham & Company.\nQuinn Bolton: Hi, guys, just a quick follow-up. I know you had a potential for an early lockup expiring Thursday morning. Just wanted to see if you guys could confirm, are we still within the 10-day measuring period, so that you could trigger that early lockup? Or does the release of second quarter results sort of in that period and we're now looking at a September 16 lockup expiration? Thank you.\nMike Tate: Yes. The release of our earnings today releases a lockup that is -- opens up on Thursday.\nQuinn Bolton: It opens on Thursday. Okay, thank you.\nJitendra Mohan: The early lockup already expired long ago.\nOperator: And there are no further questions at this time. I will turn the call back over to Leslie Green for closing remarks.\nLeslie Green: Thank you, everyone, for your participation and questions. We look forward to updating you on our progress during our Q3 earnings conference call later this fall. Thank you.\nOperator: And this concludes today's conference call. Thank you for your participation. You may now disconnect.",
        "speaker1": {
            "name": "Mike Tate",
            "content": "Thanks, Sanjay, and thanks to everyone for joining the call. This overview of our Q2 financial results and Q3 guidance will be on a non-GAAP basis. The primary difference in Astera Labs' non-GAAP metrics is stock-based compensation and its related income tax effects. Please refer to today's press release available on the Investor Relations section of our website for more details on both our GAAP and non-GAAP Q3 financial outlook, as well as a reconciliation of our GAAP to non-GAAP financial measures presented on this call. For Q2 of 2024, Astera Labs delivered record quarterly revenue of $76.9 million, which was up 18% from the previous quarter and 619% higher than the revenue in Q2 of 2023. During the quarter, we shipped products to all major hyperscalers and AI accelerator manufacturers. We recognized revenue across all three of our product families during the quarter, with the Aries product being the largest contributor. That is seen from continued momentum in AI based platforms. In Q2, Taurus revenues contributed to -- continued to primarily shift into 200-gig Ethernet-based systems, and we expect Taurus revenue to now diversify further as we begin to ship volume into 400-gig Ethernet-based systems in the third quarter. Q2 Leo revenues were largely from customers purchasing preproduction volumes for the development of their next-generation, CXL capable compute platforms, with our customers' production launch timing being dependent on the data center server CPU refresh cycle. Q2 non-GAAP gross margins was 78% and was down 20 basis points compared to 78.2% in Q1 of 2024 and better than our guidance of 77%. Non-GAAP operating expenses for Q2 were $41.2 million, up from $35.2 million in the previous quarter and consistent with our guidance. Within non-GAAP operating expenses, R&D expenses was $27.1 million; sales and marketing expense was $6.3 million and general and administrative expenses was $7.8 million. Non-GAAP operating margin for Q2 was 24.4%. Interest income in Q2 was $10.3 million. Our non-GAAP tax provision was $6.8 million for the quarter, which represents a tax rate of 23% on a non-GAAP basis. Non-GAAP fully diluted share count for Q2 was 175.3 million shares and our non-GAAP diluted earnings per share for the quarter was $0.13. Cash flow from operating activities for Q2 was $29.8 million, and we ended the quarter with cash, cash equivalents and marketable securities of just over $830 million. Now turning to our guidance for Q3 of fiscal 2024. We expect Q3 revenue to increase within a range of $95 million and $100 million, up roughly 24% to 30% sequentially from the prior quarter. We believe our Aries product family will continue to be the largest component of revenue and will be the primary driver of sequential growth in Q3, driven by growing volume deployment with our customers' AI servers. We also expect our Taurus family to drive solid growth quarter-over-quarter as design wins within new 400-gig Ethernet-based systems ramp into volume production. We expect non-GAAP margins to be approximately 75%. The sequential decline in gross margin is being driven by an expected product mix shift towards hardware solutions during the quarter. We expect non-GAAP operating expenses to be in the range of approximately $46 million to $47 million as we remain aggressive in expanding our R&D resource pool across head count and intellectual property. Interest income is expected to be approximately $10 million. Our non-GAAP tax rate should be approximately 20%, and our non-GAAP fully diluted share count is expected to be approximately 177 million shares. Adding this all up, we\u2019re expecting non-GAAP fully diluted earnings per share of a range of approximately $0.16 to $0.17. This concludes the prepared remarks. And once again, we are very much appreciative of everyone joining the call. And now we will open the line for questions. Operator? Yes, Blayne. The margins will come down to the extent we sell more hardware versus silicon. So Taurus is definitely one of those drivers. Also we do modules on the Aries side, and both -- we're seeing growth in both of those. So when you look at the growth guidance we are giving in third quarter, you have the contribution from Taurus, you have the incremental modules on Aries, but also we are seeing a lot of growth just from Aries Gen 5 going into AI servers. And a lot of new platforms, and the platforms generally are getting more content per platform. So when you look at the growth, I think it is kind of balanced between those three drivers, largely. Yes. We don't break out specifically the revenue by product. But like we said on the call, the Q2 revenues were driven heavily by the AI growth for Gen 5 and the broadening out of our design win portfolio. When you look into Q3, it's -- the three main drivers are the initial Taurus ramp into 400 gig, the broadening out of AI servers for Gen 5 in both merchant as well as internally developed accelerator programs, and then also we are doing back-end clustering with our Aries SCM modules. So when you look at that, those three drivers are mainly giving us the growth in Q3. Yes. Taurus predominantly is modules. Aries, we are doing the back-end clustering of GPUs with the modules predominantly, but the bulk of the revenues is stand-alone retimers in that product family. Leo, once it ramps, we\u2019ll do add-in cards and silicon, but they\u2019ll be heavily skewed towards silicon. Yes. The release of our earnings today releases a lockup that is -- opens up on Thursday."
        },
        "speaker2": {
            "name": "Jitendra Mohan",
            "content": "Thank you, Leslie. Good afternoon, everyone, and thanks for joining our second quarter conference call for fiscal 2024. AI continues to drive a strong investment cycle, as entire industries look to expand their creative output and overall productivity. The velocity and dynamic nature of this investment in AI infrastructure, is generating highly complex and diverse challenges for our customers. Astera Lab's intelligent and flexible connectivity solutions are developed ground up to navigate these fast-paced, complicated deployments. We are working closely with our hyperscaler customers to help them solve these challenges across diverse AI platform architectures that features both third-party and internally developed accelerators. In addition to these favorable secular trends, we are also benefiting from new company specific product cycles across multiple technologies, which will also contribute to our growth in the form of higher average silicon content per AI platform. A strong leadership position and great execution by our team resulted in record revenue for Astera Labs in the June quarter supports our strong outlook for the third quarter and gives us confidence in our ability to continue outperforming industry growth rates. Astera Labs delivered strong Q2 results, setting our first consecutive record for quarterly revenue, strong non-GAAP operating margin and positive operating cash flows. Our revenue in Q2 was $76.9 million up 18% from the previous quarter and up 619% from the same period in 2023. Non-GAAP operating margin was 24.4%, and we delivered $0.13 of non-GAAP diluted earnings per share. Operating cash flow generation was also strong during the quarter, coming in at $29.8 million. With continued business momentum and a broadening set of growth opportunities, we are investing in our customers by rapidly scaling the organization. During the quarter, we expanded our Cloud-Scale Interop Lab to Taiwan and announced the opening of a new R&D center in India. We also announced the appointment of Bethany Mayer to our Board of Directors, bringing additional strategic leadership to the company. Today, Astera Labs is focused on three core technology standards; PCI Express, Ethernet, and Compute Express Link. We are shipping three separate product families supporting these different connectivity protocols, all generating revenue and in various stages of adoption. Let me touch upon our business with each of these product families and how we support them with our differentiated architecture and COSMOS software suite. Then I will turn the call over to Sanjay to dive deeper into our growth strategy. Finally, Mike will provide additional details on our Q2 results and our Q3 financial guidance. First, let us talk about PCI Express. During the quarter, we saw continued strong demand for our Aries product family to drive reliable PCI Gen 5 connectivity in AI systems by delivering robust signal integrity and link stability. While merchant GPU suppliers drove early adoption of PCI Gen 5 into real systems over the past year, we are now also seeing our hyperscaler customers introduce and ramp new AI server programs based upon their internally developed accelerators utilizing PCI Gen 5. Looking ahead, AI accelerator processing power is continuing to increase at an incredible pace. The next milestone for the AI technology evolution is the commercialization of PCI Gen 6, which doubles the connectivity bandwidth within AI servers, creating new challenges for link reach, reliability and latency. Our Aries 6 PCI Retimers family helps to solve these challenges with the next generation of our software-defined architecture, offering a seamless upgrade path to a widely deployed and field-tested Gen 5 solutions. We have started shipping initial quantities of preproduction orders of our PCIe Gen 6 solution, Aries 6. We ship and support our hyperscaler customers initial program developments that are based on Nvidia's Blackwell platform, including GB200. We look forward to supporting more significant production ramps in the quarters to come. Next let us talk about Ethernet. Our portfolio of Taurus Ethernet smart cable modules helps relieve connectivity bottlenecks by overcoming reach, signal integrity and bandwidth issues by enabling robust 100-gig per lane connectivity over copper cables or AEC. Today, we are pleased to announce that our 400-gig Taurus Ethernet SCMs have shifted into volume production, with an expected ramp through the back half of 2024. This ramp is happening across multiple platforms in multiple cable configurations, and we are working with multiple cable partners to support the expected volumes. Taurus will be ramping across a multitude of 400-gig applications to scale out connectivity on both AI compute platforms, as well as general purpose compute systems. We are excited about the breadth and diversity of our Taurus design wins and expect the product family to be accretive to our corporate growth rate going forward. Next is Compute Express Link, or CXL. We continue to work closely with our hyperscaler customers on a variety of use cases and applications for CXL. In Q2, we shipped material volume of our Leo products for preproduction large-scale deployment in data centers. We expect to see data center platform architects utilize CXL technology to solve memory bandwidth and capacity bottlenecks using our Leo family of products. The initial deployments are targeting memory expansion use cases, with production ramps starting in 2025 when new CXL-capable CPUs are broadly available. Finally, I\u2019d like to spend a moment on COSMOS, which is a software platform that brings all of our product families together. We have discussed how COSMOS not only runs on our chips, but also in our customers' operating stacks to deliver seamless customization, optimization and monitoring. The combination of our semiconductor and hardware solutions with COSMOS software enables our product to become the eyes and ears of connectivity infrastructure, helping fleet managers to ensure their AI and cloud infrastructure is operating at peak utilization. By improving the efficiency of their data centers, our customers are able to generate higher ROI and reduce downtime. To summarize sustained secular trends in AI adoption, design wins across diverse AI platforms at hyperscalers, featuring both third-party and internally developed accelerators an increasing average dollar content in next-generation GPU-based AI platforms gives us confidence in our ability to outperform industry growth rates. With that, let me turn the call over to our President and COO, Sanjay Gajendra to discuss some of our recent product announcements and our long-term growth strategy. Harlan, thank you so much for the question. It is great to be in the place that we are here today. We feel very confident about what is to come. Clearly we don't guide more than one quarter out, so please don't take that as any guidance. But we really believe that we are in the early innings of AI here. All of the hyperscalers are increasing their CapEx targets for the rest of the year. 2025 is expected to be even higher. We heard that the Llama model requires 10 times more compute in order to solve that. So all of these trends are basically driving a radical shift in technology. We are seeing, as you correctly pointed out, a lot of our hyperscaler customers ramp their internally developed AI accelerators in addition to deploying third-party AI accelerators. And we are very pleased that we have designed wins across all of these different platforms. Our customers are ramping their platforms and we are ramping multiple product families. As Sanjay mentioned, both Aries and Taurus are ramping into these new platforms. So we feel very good about what is in store for the future and feel that with the rising content on a per GPU basis, we\u2019ll be able to outpace the market growth in the long-term. Blayne, it is a great observation. Let me say that as these compute platform gets more and more powerful to address these growing AI models, the only way to keep them fed, to keep these GPUs utilized is to get more and more data in and out of these platforms. So in the past, the CPUs played a very central role in terms of being the can't do it for all of this information. But with the new accelerated compute architecture, CPU is largely orchestration of a control engine for the most part, it does do a few other things. But in general, you are trying to get the data in and out of the GPU, using the scale out and scale up networks that are made up of either PCI Express, Ethernet or NVLink protocols. And as these protocols go faster and faster, we end up seeing more and more demand for the product that we have. And as a result, as these new systems get deployed, we see higher content for us on a per GPU basis. and it's largely to improve the GPU utilization through these increased data rates. Let me take that. This is Jitendra, Tom. The architectures for AI systems are definitely evolving. And actually, I would say, they are evolving at a very rapid pace. Different customers use different architectures to craft their systems. If you look at Nvidia-based systems, they do use NVLink, which is, of course, a proprietary closed interface. The rest of the world largely uses protocols that are either PCI Express or Ethernet or they are based on PCI Express and Ethernet. And the choice of particular protocol is really dependent upon the infrastructure that the hyperscalers have and how they choose to deploy this technology. Clearly, we play in both. Our Taurus Ethernet Smart Cable Module support Ethernet. And now with our Aries Smart Cable Modules, we are able to support our PCI Express as well. And if you think about the evolution, we started with Aries retimers for driving mostly within the box connectivity and shorter distance connectivity over passive cables. As these networking architecture evolved and you needed to cluster more GPUs together, we went with the Aries smart cable modules that allow you to connect multiple racks together, up to 7 meters of copper cables. And as it expands into even further distances, we go into optical where we demonstrated running a very popular GPU over 50 meters of optical fiber. So these are all of the tools that we are making available to our hyperscaler partners for them to craft their solutions and deploy AI at the data center scale.\nThomas O\u2019Malley: Helpful. As a follow-up, I know this is a bit of a tougher question, but I do think that there is a lot of confusion out there. And I just would appreciate your thoughts. You mentioned in the prepared remarks hundreds of different types of deployment styles for the GB200. Obviously, certain hyperscalers are going to do it their way and then certain hyperscalers are going to take what's called the kind of entire system, so the $36 million of the $72 million. Can you talk about your assumptions for what you think will be the percentage that goes towards the full system and then kind of towards the hyperscalers that use kind of their own methods and talk about the content opportunities if they would kind of play out in those two scenarios? I do think that Nvidia and others are talking potentially about more systems than historical, but just maybe the puts and takes upon how different hyperscalers or architect systems and what it means for your content. Thank you. Yes. Great questions. And as you pointed out, a lot of moving pieces obviously, right? But here is, I think, what we know and what we can comment on. First of all, all the hyperscalers are indeed deploying new AI platforms that are based on merchant silicon or third-party accelerators, as well as their own accelerators. And overall, we do expect our retimer content to go up. Now if you double-click specifically on Nvidia or the Blackwell system, it comes in many, many different flavors. If you think about the overall Blackwell platform, it is really pushing the technology boundaries. And what that is doing is it's creating more challenges for the hyperscalers, whether it is power delivery or thermals, software complexities or connectivity. As these systems grow bigger, they run faster, become more complex, we absolutely think that the need for retimer goes up. And that drives our content higher on a per GPU basis. Now it is harder to predict which particular platform will have what kind of share. That's not really our business to predict. What we are doing is we are supporting our customers, our AI platform providers as well as hyperscalers to make sure that these kind of high-tech platforms can be deployed as easily as possible. And at the end of the day, what you will find is hyperscalers will have to either adopt their data centers to these new technologies or they'll have to adopt this new technology to their data centers. And that creates a great opportunity for our products. We already have design wins across multiple form factors of hyperscaler GPUs as well as the third-party GPUs. And overall, we expect our business to continue to grow strongly. Very exciting times for us.\nThomas O'Malley: Thank you very much. Yes. Thank you, Quinn. Unfortunately, it will not be appropriate for us to kind of comment on rumors and third-party information that seems to be circulating around. What we will say is that we are committed to whatever platform our customers want to deploy. Whether it's a full rack or it's an HGX server or something in between, we are working with them very, very closely, shoulder to shoulder every day. As Sanjay mentioned, we already have multiple design wins in the Blackwell family including the GB200. We are shipping initial quantities of preproduction to the early adopters. And we do have backlog in place that serves the Blackwell platform, including GB200. Yes. Good question. So the 800-gig timing, we believe, is going to be late in 2025. Right now, what we are seeing is 400-gig applications for some of the AI systems as well as actually we are seeing them for general purpose compute as well where you are doing the traditional server to top of the rack connection. So that will continue on for the rest of this year for 400 gig deployments. And then as we get some of the newer mix that are capable of 100 gig and 200 gig per lane, et cetera are try to get to 800 gig is where we see broadening of this market and more deployments across different hyperscalers across different platforms in the latter half of 2025. I would say it's a little bit hard to generalize. It varies quite a bit. Even one particular platform, you can have different form factors. Even if you look at, let's say, Blackwell, you have HGX, you have MGX, you have NVLs, you have custom racks that are getting deployed. And if you look at each one of them, you'll find different amount of content. Number of retimers will vary where they get placed. It will vary -- but what is very consistent is that the overall content does go up for us. Now the other factor to consider is the choice of back-end network. Again, for example if you look at the Blackwell family, they use NVLink, which is a closed proprietary standard, which we do not participate in. But when somebody uses a PCI Express or PCI Express-based protocol at their back-end connectivity, then our content goes up pretty significantly because now we are shipping not only our retimers but also the smart cable -- Aries smart cable modules into that application. Similarly, if the back-end interconnected Ethernet, that will benefit our Taurus family of product lines. So it really varies greatly on what the architecture is of the platform and what form factor is getting deployed in. I mean if you look at the Blackwell family, it's like a marvel of technology. The amount of content that is being pushed into that platform is incredible. And as I mentioned earlier, that does create other problems, right? There is so much compute packed in such small space. They're delivering power to those racks -- to those GPUs themselves and the CPUs is the challenge. How to cool them, it becomes a challenge. And the fact that modern data centers are just not equipped to handle many of these issues. So what the hyperscalers are doing is they're taking these broad platforms in our technology and trying to adapt it so that it fits into their data centers. And that's where we see a lot of opportunity for our existing products, the ones that we have talked about, as well as some new products that we've been working on again, shoulder to shoulder with our hyperscaler and air platform customers. So very excited to see how these new platforms will get rolled out, including Blackwell, including the hyperscaler internal AI platforms and the increased content that we have there. The early lockup already expired long ago."
        },
        "speaker3": {
            "name": "Sanjay Gajendra",
            "content": "Thanks, Jitendra and good afternoon, everyone. We are pleased with our robust Q2 results and strong top-line outlook for Q3. But we are even more excited about the volume and breadth of opportunities that lie ahead. Today, I will focus on five growth vectors that we believe will help us to grow our business faster than industry growth rates over the long term. First, Astera Labs is in a unique position with design wins across diverse AI platform architectures featuring both third-party and internally developed accelerators. This diversity gives us multiple paths to grow our business. This hybrid approach of using third-party and internally developed accelerators allows hyperscalers to optimize their fleet to support unique workload requirements and infrastructure limitations, while also improving capital investment efficiency. Our intelligent connectivity platform with its flexible software-based architecture enables portability and seamless reuse between platforms while creating growth opportunities for all our product families. In addition to the third-party GPU platforms, we also expect to see several large deployments based on internally developed AI accelerators hitting production volume over the next few quarters and driving incremental PCIe and Ethernet volumes for us. Second, we see increasing content on next-generation AI platforms. Nvidia's Blackwell GPU architecture is particularly exciting for us, as we expect to see strong growth opportunities based on our design wins as hyperscalers compose solutions based on Blackwell GPUs, including GB200 across their data center infrastructure. To support various AI workloads, infrastructure challenges, software, power and cooling requirements, we expect multiple deployment variants for this new GPU platform. For example, Nvidia cited 100 different configurations for Blackwell in their most recent earnings call. This growing trend of complexity and diversity presents an exciting opportunity for Astera Labs as our flexible silicon architecture and COSMOS software suite can be harnessed to customize the connectivity backbone for a diverse set of deployment scenarios. Overall, we expect our business to benefit from the Blackwell introduction with higher average dollar content of our products per GPU, driven by a combination of increasing volumes and higher ASPs. The next growth vector is the broadening applications and use cases for our Aries product family. Aries is in its third generation now and represents the gold standard for PCIe Retimers in the industry. The introduction of the new Aries 6 Retimers built upon the company's widely deployed and battle-tested PCIe 5 retimers and the industry transition to PCIe Gen 6 will be a catalyst for increasing PCIe retimer content for Astera. Our learnings from hundreds of design wins and production deployment over the last several years enables us to quickly deploy PCIe Gen 6 technology at scale. As Jitendra noted, we\u2019re now shipping initial quantities of preproduction volume for Aries 6 and currently have meaningful backlog in place to support the initial deployment of hyperscaler AI servers featuring Nvidia's Blackwell GPUs including GB200. We're also very excited about the incremental PCIe connectivity market expansion that will be driven by multi-rack GPU clustering. Similar to the dynamic within the Ethernet AEC business, the reach limitations of passive PCIe copper cables are a bottleneck for the number of GPUs that can be clustered together. Our purpose-built Aries smart cable modules solve these issues by providing robust signal-integrity and link stability over materially longer distances improving rack airflow, while actively monitoring and optimizing link health. This PCIe AEC opportunity is in the early stages of adoption and deployment and we view the multi-rack GPU clustering application as a new and growing market opportunity for our Aries product family. In June, we announced the industry's first demonstration of end-to-end PCIe optical connectivity to provide unprecedented reach for larger GPU clusters. We are proud to broaden our PCIe leadership once again by demonstrating robust PCIe links over optical interconnects between GPUs, CPUs, CXL memory devices and other PCIe endpoints. This breakthrough expands our intelligent connectivity platform to allow customers to seamlessly scale and extend high-bandwidth, low-latency PCI interconnects over optics. Overall, we expect our Aries PCIe retimer business to deliver strong growth as system complexity, platform diversity and speeds continue to increase and on average, result in higher retimer content per GPU in next-generation AI platforms. Next in addition to the strong growth prospect of our Aries product family across the PCIe ecosystem, we are also seeing our Taurus product family for Ethernet AEC application start to meaningfully contribute to the growth in the back half of 2024. What is exciting about these ramps is the diversity in application and use cases. We are seeing demand for our Taurus product family for both AI and general compute platforms. We\u2019re supporting the market with multiple cable configurations, including straight, Y cables and X cables. We will be shipping volume into hyperscaler build-outs, supporting multiple cable vendors to enable a diverse supply chain that is crucial for hyperscalers. Overall, we are very excited about Taurus becoming yet another engine of growth as we look to expand the top-line while also diversifying our product family contributions. Last but not least, CXL is an important technology to solve memory bandwidth and capacity bottlenecks in compute platforms. We are working closely with our hyperscaler partners to demonstrate various use cases for this technology and starting to deploy our Leo CXL controllers in preproduction racks in data centers. We have incorporated the learnings, customization and security requirements into our COSMOS software and have the most robust, cloud-ready CXL solution in the industry. We have demonstrated that our Leo CXL Smart Memory Controllers improve application performance and reduce TCO in compute-platforms. Very importantly, we can accomplish many of these performance gains with zero application-level software changes or upgrades. Overall, we remain very excited about the potential of CXL in data center applications. Finally, our close collaboration and front-row seat with hyperscalers, and AI platform providers continues to yield valuable insights regarding the direction of compute technologies and the connectivity topologies that will be required to support them. This close collaboration is helping us identify new product and business opportunities and additional engagement models across our entire intelligent connectivity platform, which we believe will drive strong, long-term growth for Astera. With that, I\u2019ll turn the call over to our CFO Mike Tate, who will discuss our Q2 financial results and our Q3 outlook. Absolutely, Harlan. Sanjay here. Good to hear your voice. Yes. So in general, that's a correct statement. We have several design wins on the compute side. Just for reasons like you highlighted, either SSDs not being Gen 5-ready or for dollars being sort of factoring into the AI platforms. There has been a slower than expected growth on the general compute. But at some point, like we keep saying, the servers are going to fall off the rack at some point given how long they've been in the fleet. So we do expect the general compute to start picking up, especially as both AMD and Intel get to production with the Turin and the kind of Granite Rapids based CPUs. So overall 2025, we do expect that the compute platform will start figuring in terms of being meaningful revenue growth. Like I noted, we do have design wins already in these platforms for Aries retimers. But also I would like to add that we do have design wins for our Taurus Ethernet module application as well in general compute. So we should see sort of the two-engine growth story along the general compute to go along with all of the things we shared on AI, both for third-party or merchant GPUs, as well as the big change that we are seeing now is the ramp in the internally developed accelerators, those things being a meaningful and a significant driver for our growth. Yes, absolutely. Let me take that, Joe. So overall, this is a big and growing market. I think that fact is clear. I mean the fact that you have larger names jumping into the mix sort of validates the market that the retimer represents. Now a couple of points to keep in mind is that connectivity products, especially PCI Express, tends to have a certain nuance to it which is the fact that we are the device in the middle. We are always in between GPU, storage, networking and so on. And interoperation, especially at high volume, cloud-scale deployment becomes critical. So what we have done in the last three years, four years is really work shoulder-to-shoulder with our hyperscaler and AI platform providers to ensure that the interoperation is met, the platform-level deployment, whether it is diagnostic, telemetry, firmware management is all addressed, including the COSMOS software that we provide from a management -- fleet management and diagnostic type of capability. Those all have been integrated into our customers' operating stack. So in general, the picture I'm trying to paint here is that the tribal knowledge that we have built, the penetration that we have not just with the silicon, but also software does give us a significant advantage compared to our competitors. Now having said that, we will continue to work hard. We have several design wins for PCIe Gen 6 like we shared in today's call that are all designed around the next-generation GPU platform, specifically the Blackwell-based GPUs from Nvidia, which I publicly noted to support Gen 6. So we'll continue to work through them. We are currently shipping preproduction volume to support some of the initial ramps, including for GB200 based platforms. So overall, we feel good about the position that we are in, both in terms of Gen 5, as well as transitioning those designs into Gen 6 as the platforms develop and grow. Blayne, if I can add one more point. You didn't quite directly asked us for the September quarter growth. I do want to be abundantly clear on one point, which is the growth that we are forecasting for September quarter is based upon not just the power sampling, but all of the additional production ramps that we are seeing for both the third-party platforms, but also internally developed accelerators. That is what is modeling and driving the growth that we are highlighting for September although there are maybe other things that you can look at the overall stuff. Yes. So in terms of material ramp, it's a function of CPUs being available that supports CXL 2.0. So we are of course, tracking the announcements from AMD and Intel to essentially get to production in the second half of this year for Turin and Granite Rapids. So in general, these things will take a little time in terms of engineering those things into platforms. So what we are guiding is 2025 is when we expect production ramps to pick up on CXL. Yes. Good question. Let me take that. So if you think about AECs, in general, 800 gig, where you're running 100 gig per lane is the first broad use case that we believe for AEC applications. If you look at [data] (ph) rates lower than that, let's say, 400 gig and so on, it tends to be very, frankly, case by case. It depends on the topology, application and so on. So the good thing about the design wins we have is that these scale across multiple platforms both from an AI and general compute standpoint and supporting various different topologies. And the revenue drivers that we are essentially highlighting for 3Q and beyond is based on supporting these applications. With 800 gig, it becomes much more broader with several different customers essentially requiring AECs. Exactly. It depends on the topology for what it is in terms of how the back-end networks are designed for the AI subsystems. In general all of this -- when it comes to active cabling type of technology, it becomes case by case depending on the infrastructure and how exactly systems are being put together compared to a component like a PCIe retimer, that goes across a broad array of use cases across multiple different deployment scenarios. So that's the nuance to keep in mind when you look at AEC markets. And so the volume and the deployment scale tends to be very broad, right, if you are looking at how infrastructures are being put together. So it is one of those things where you look at case by case. But as long as you're able to address a wide variety of applications, it does very significantly add up. Let me try to add a little bit more color on that. But before I do that, let me give you and remind two data points we've already covered in the Q&A so far. First point, let us be very clear that our PCIe retimer content per GPU, on average will continue to grow as the AI systems scales across various different topologies. And this applies to both third-party, like standard merchant GPUs as well as internally developed GPUs. The second reminder that I want to kind of note is that specifically for Blackwell, we expect our PCIe content per GPU to go up. Now what you are asking is specifically about the deployment scenarios, which right now is evolving, right? So we have design wins for several different topologies, including the GB200. But if you look at the various different options that Nvidia is offering and how those are being composed and considered by the hyperscalers, that situation is evolving at the moment. The key message that we want to deliver is that, overall, our PCIe content is going to be higher than the Hopper generation. We expect that the design wins that we are starting to see and we're starting to ship from a pre-production standpoint are all meaningful that will essentially allow us to continue to have a robust growth engine, as far as our PCIe retimer business is concerned. Yes, it's probably both, to be honest with you, in the sense that we do see it, it's no secret, right? I think many of the hyperscalers are doing their own accelerators, which are driven by the workloads or the business models that they pursue. I think that will continue as a macro trend in terms of internally developed accelerators going hand in hand with GPUs that are available from Nvidia or AMD or others. So that's the model that we believe will be here to stay, that hybrid approach. And for us really, the reason we are highlighting is that, of course, we have had a significant business that has grown in the last year or two years from the designs that we have been supporting with the merchant GPU deployments that have happened. But at the same time, now we're reaching a point where the accelerator volumes are also starting to ramp up. And for us the good news is that we are on all the major AI accelerator platforms from a design win standpoint or at least all the major ones that are out there. And for us, we have multiple parts to grow our business, and that is a very positive thing that we believe will continue to allow us to keep delivering the kind of reserves that we're doing. And as new CPU/GPU architectures come about, just like the Nvidia's Blackwell platform, we do expect to gain from it both on the retimer content as well as other products that we can service to this space. I'm trying to see how best to answer that question. So you got to -- let me try to provide some color. The design win windows is, whatever, for these platforms, it's you're looking at -- once GPUs become available, you're looking at 6 to 12 months before they go to production. So that's one thing to keep in mind. But also please also think about how hyperscalers go about doing their stuff, right? Everyone is in an arms' race right now getting to production as quickly as possible. In many different situations resources are also limited. Meaning for every 10 engineers that they may need, they might have two or three, just given the number of platforms and how quickly everyone is trying to move. And to that standpoint, what is happening is that many of these engineers are familiar with our Gen 5 retimers. They've designed it across multiple platforms. They've built software tools and capabilities around it. And now our Gen 6 retimers are essentially a seamless upgrade from a software standpoint, from a hardware standpoint. So it does offer the lowest risk and fastest path to our customers. And that plays well within their own objectives of trying to get something out quickly and dealing with resources that might not be available at the levels that are required. So overall we\u2019re starting to gain from it, and we are essentially being the leader in the space, being the one that is getting the first crack at these opportunities. And we are doing everything we can to convert those things into design wins and revenue."
        }
    },
    {
        "symbol": "ALAB",
        "quarter": 1,
        "year": 2024,
        "date": "2024-05-08 09:22:10",
        "content": "Operator: Thank you for standing by. My name is Regina, and I will be your conference operator today. At this time, I would like to welcome everyone to the Astera Labs First Quarter 2024 Earnings Conference Call. All lines have been placed on mute to prevent any background noise. After management remarks, there will be a question-and-answer session. [Operator Instructions] I will now turn the call over to Leslie Green, Investor Relations for Astera Labs. Leslie, you may begin.\nLeslie Green: Thank you, Regina. Good afternoon, everyone, and welcome to the Astera Labs first quarter 2024 earnings call. Joining us today on the call are Jitendra Mohan, Chief Executive Officer and Co-Founder; Sanjay Gajendra, President, Chief Operating Officer and Co-Founder; and Mike Tate, Chief Financial Officer. Before we get started, I would like to remind everyone that certain comments made in this call today may include forward-looking statements regarding, among other things, expected future financial results, strategies and plans, future operations and the markets in which we operate. These forward-looking statements reflect management's current beliefs, expectations and assumptions about future events, which are inherently subject to risks and uncertainties that are discussed in detail in today's earnings release and in the periodic reports and filings we file from time to time with the SEC, including the risks set forth in the final perspective relating to our IPO. It is not possible for the company's management to predict all risks and uncertainties that could have an impact on these forward-looking statements or the extent to which any factor or combination of factors may cause actual results to differ materially from those contained in any forward-looking statement. In light of these risks, uncertainties and assumptions, the results, events or circumstances reflected in the forward-looking statements discussed during this call may not occur and actual results could differ materially from those anticipated or implied. All of our statements are made based on information available to management as of today, and the company undertakes no obligation to update such statements after the day of this call to conform to these as a result of new information, future events or changes in our expectations, except as required by law. Also during this call, we will refer to certain non-GAAP financial measures, which we consider to be an important measure of the company's performance. These non-GAAP financial measures are provided in addition to and not as a substitute for or superior to financial results prepared in accordance with U.S. GAAP. A discussion of why we use non-GAAP financial measures and reconciliations between our GAAP and non-GAAP financial measures is available in the earnings release we issued today, which can be accessed through the Investor Relations portion of our website and will also be included in our filings with the SEC, which will also be accessible through the Investor Relations portion of our website. With that I would like to turn the call over to Jitendra Mohan, CEO of Astera Labs. Jitendra?\nJitendra Mohan: Thank you, Leslie. Good afternoon, everyone, and thanks for joining our first earnings conference call as a public company. This year is off to a great start with Astera Labs seeing strong and continued momentum along with the successful execution of our IPO in March. First and foremost, I would like to thank our investors, customers, partners, suppliers and employees for their steadfast support over the past six years. We have built Astera Labs from the ground up to address the connectivity bottlenecks to unlock the full potential of AI in the cloud. With your help, we've been able to scale the company and deliver innovative technology solutions to the leading hyperscalers and AI platform providers worldwide. But our work is only just beginning. We are supporting the accelerated pace of AI infrastructure deployments with leading hyperscalers by developing new product categories, while also exploring new market segments. Looking at industry reports over the past several weeks, it is clear that we remain in the early stages of a transformative investment cycle by our customers to build out the next generation of infrastructure that is needed to support their AI roadmaps. According to recent earning reports, on a consolidated basis, CapEx spend during the first quarter for the four largest U.S. hyperscalers grew by roughly 45% year-on-year to nearly $50 billion. Qualitative commentary implies continued quarterly growth in CapEx for this group through the balance of the year. This is truly an exciting time for technology innovators within the cloud and AI infrastructure market, and we believe Astera Labs is well position to benefit from these growing investment trends. Against the strong industry backdrop, Astera Labs delivered strong Q1 results with record revenue, strong non-GAAP operating margin, positive operating cash flows, while also introducing two new products. Our revenue in Q1 was $65.3 million up 29% from the previous quarter and up 269% from the same period in 2023. Non-GAAP operating margin was 24.3%, and we delivered $0.10 of pro forma non-GAAP diluted earnings per share. I will now provide some commentary around our position in this rapidly evolving AI market. Then I will turn the call over to Sanjay to discuss new products and our growth strategy. Finally, Mike will provide additional details on our Q1 results and our Q2 financial guidance. Complex AI model sizes continue doubling about every six months, fueling the demand for high performance AI platforms running in the cloud. Modern GPUs and AI accelerators are phenomenally good at compute, but without equally fast connectivity, they remain highly underutilized. Technology innovation within the AI Accelerator market has been moving forward at an incredible pace and the number and variety of architectures continues to expand to handle trillion parameter models, while improving AI infrastructure utilization. We continue to see our hyperscaler customers utilize the latest merchant GPUs and proprietary AI accelerators to compose unique data center scale AI infrastructure. However, no two clouds are the same. The major hyperscalers are architecting their systems to deliver maximum AI performance based on the specific cloud infrastructure requirements, from power and cooling to connectivity. We are working alongside our customers to ensure these complex and different architectures achieve maximum performance and operate reliably even as data rates continue to double. As the systems continue to move data faster and grow in complexity, we expect to see our average dollar content per AI platform increase and even more so with the new products we have in development. Our conviction in maintaining and strengthening our leadership position in the market is rooted in our comprehensive intelligent connectivity platform and our deep customer partnerships. The foundation of our platform consists of semiconductor based and software-defined connectivity ICs, modules and boards, which all support our COSMOS software suite. We provide customers with a complete customizable solution, tips, hardware and software, which maximizes flexibility without performance penalties, delivers deep fleet management capabilities and matches space with the ever quickening product introduction cycles of our customers. Not only does COSMOS software run on our entire product portfolio, but it is also integrated within our customers' operating stacks to deliver seamless customization, optimization and monitoring. Today, Astera Labs is focused on three core technology standards: PCI Express, Ethernet and Compute Express Link. We're shipping three separate product families, all generating revenue and in various stages of adoption and deployment supporting these different connectivity protocols. Let me touch upon each of these critical data center connectivity standards and how we support them with our differentiated solutions. First, PCI Express. PCIe is the native interface on all AI accelerators, TPUs and GPUs, and is the most prevalent protocol for moving data at high bandwidth and low latency inside servers. Today, we see PCIe Gen 5 getting widely deployed in AI servers. These AI servers are becoming increasingly complex. Faster signal speeds in combination with complex server topologies are driving significant signal integrity challenges. To help solve these problems, our hyperscalers and AI accelerator customers utilize our PCIe Smart DSP Retimers to extend the reach of PCIe Gen 5 between various components within heterogeneous compute architecture. Our Aries product family represents the gold standard in the industry for performance, robustness and flexibility, and is the most widely deployed solution in the market today. Our leadership position with millions of critical data links running through our Aries Retimers and our COSMOS software enables us to do something more, become the eyes and ears to monitor the connectivity infrastructure and help fleet managers ensure their AI infrastructure is operating at fleet utilization. Deep diagnostics and monitoring capabilities in our chips and extensive fleet management features in our COSMOS software, which are deployed together in our customer's fleet has become a material differentiator for us. Our COSMOS software provides the easiest and fastest path to deploy the next generation of our devices. We see AI workloads and newer GPUs driving the transition from PCIe Gen 5 running at 32 gigabits per second per lane to PCIe Gen 6 running at 64 gigabits per second per lane. Our customers are evaluating our Gen 6 solutions now, and we expect them to make design decisions in the next six to nine months. In addition, while we see our Aries devices being heavily deployed today for interconnecting AI accelerators with CPUs and networking, we also expect our Aries devices to play an increasing role in backend fabrics, interconnecting AI Accelerators to each other in AI clusters. Next, let's talk about Ethernet. Ethernet protocol is extensively deployed to build large scale networks within data centers. Today, Ethernet makes up the vast majority of connections between servers and top of rack switches. Driven by AI workloads' insatiable need for speed, Ethernet data rates are doubling roughly every two years, and we expect the transition from 400 gig Ethernet to 800 gig Ethernet to take place later in 2025. 800 gig Ethernet is based on 100 gigabits per second per lane signaling rate, which is facing tremendous pressure on conventional passive cabling solutions. Like our PCIe Retimers, our portfolio of Taurus Ethernet Retimers helps relieve these connectivity bottlenecks by overcoming the reach, signal integrity and bandwidth issues by enabling robust 100 gig per lane connectivity over copper. Unlike our Aries portfolio, which is largely sold in a chip format, we sell our Taurus portfolio largely in the form of smart cable modules that are assembled into active electrical cables by our cable partners. This approach allows us to focus on our strength and fully leverage our COSMOS software suite to offer customization, easy qualification, deep telemetry and field upgrade to our customers. At the same time, this model enables our cable partners to continue to excel at bringing the best cabling technology to our common end customers. We expect 400 deployments based on our Taurus smart cable modules to begin to ramp in the back half of 2024. We see the transition to 800 gig Ethernet starting to happen in 2025, resulting in broad demand for AECs to both scale up and scale out AI infrastructure and strong growth for our Taurus Ethernet Smart Cable module portfolio over the coming years. Last is Compute Express Link or CXL. CXL is a low latency cash coherent protocol, which runs on top of PCIe protocol. CXL provides an open standard for disaggregating memory from compute. CXL allows you to balance the memory bandwidth and capacity requirements independently from compute requirements, resulting in better utilization of compute infrastructure. Over the next several years, data center platform architects plan to utilize CXL technology to solve memory bandwidth and capacity bottlenecks that are being exacerbated by the exponential increase in compute capability of CPUs and GPUs. Major hyperscalers are actively exploring different application of CXL memory expansion. While the adoption of CXL technology is currently in its infancy, we do expect to see increased deployments with the introduction of next generation CXL capable datacenter server CPUs such as Granite Rapids, Turing and others. Our first to market portfolio of Leo CXL memory connectivity controllers is very well positioned to enable our customers to overcome memory bottlenecks and deliver significant benefits to their end customers. We have worked closely with our hyperscaler customers and CPU partners to optimize our solution to seamlessly deliver these benefits without any application level software changes. Furthermore, we have used our COSMOS software to include significant learnings we have had over the last 18 months and to customize our Leo memory expansion solution to the different requirements from each hyperscaler. We anticipate memory expansion will be the first high volume use case that will drive design wins into volume production in 2025 timeframe. We remain very excited about the potential of CXL in datacenter applications and believe that most new CPUs will support CXL and hyperscalers will increasingly deploy innovative solutions based on CXL. With that, let me turn the call over to our President and COO, Sanjay Gajendra, to discuss some of our recent product announcements and our long-term growth strategy.\nSanjay Gajendra: Thanks, Jitendra, and good afternoon, everyone. Astera Labs is well positioned to demonstrate long-term growth through a combination of three factors. One, we have a strong secular tailwinds with increased AI infrastructure investment. Two, the next generation of products within existing product lines are gaining traction. And third, the introduction of new product lines. Over the past three months, we announced two new and significant products that play an important role in enabling next generation AI platforms and provide incremental revenue opportunities as early as the second half of 2024. First, we expanded our widely deployed field proven Aries Smart DSP Retimers portfolio with the introduction and public demonstration of our Aries 6 PCIe Retimer that delivers robust, low power PCIe Gen 6 and CXL 3 connectivity between next generation GPUs, AI accelerators, CPUs, NICs, and CXL memory controllers. Aries 6 is the third generation of our PCIe Smart Retimer portfolio and provides the bandwidth required to support data intensive AI workloads while maximizing utilization of next generation GPUs operating at 64 gigabit per second per link. Fully compatible with our field deployed COSMOS software suite, Aries 6 incorporates the tribal knowledge we have acquired over the past four years by partnering and enabling hyperscadeless to deploy AI infrastructure in the cloud. Aries 6 also enables the seamless upgrade path from current PCIe Gen 5 based platforms to next generation PCIe Gen 6 based platforms for our customers. With Aries 6, we demonstrated industry's lowest power at 11 watts at Gen 6 in full 16 lane configuration running at 64 gigabit per second per lane, significantly lower than our competitors and even lower than our own Aries Gen 5 Retimer. Through collaboration with leading providers of GPUs and CPUs such as AMD, ARM, Intel, and NVIDIA, Aries 6 is being rigorously tested at Astera's Cloud-Scale Interop Lab and in customers' platforms to minimize interoperation risk, lower system development cost, and reduce time to market. Aries 6 was demonstrated at NVIDIA's GTC event during the week of March 18th. Aries 6 is currently sampling two leading AI and cloud infrastructure providers, and we expect initial volume ramps to begin in 2025. We also announced the introduction and sampling of our Aries PCIe and CXL Smart Cable Modules for Active Electrical Cables or AECs to support robust and long reach, up to 7 meters copper cable connectivity. This is 3x the standard reach defined in the PCIe spec. Our new PCIe AEC solution is design for GPU clustering application by extending PCIe backend fabric deployments to multiple racks. This new Aries product category expands our market opportunity from within the rack to across racks. As with our entire product portfolio, Aries Smart Cable Modules support our COSMOS software suite to deliver a powerful yet familiar array of link monitoring, fleet management and rack tools which are customizable for diverse needs of our hyperscaler customers. We leveraged our expertise in silicon, hardware and software to deliver a complete solution in record time and we expect initial shipments to begin later this year for the PCIe AECs. We believe this new Aries product announcement represents another concrete example of Astera Labs driving the PCIe ecosystem with technology leadership with an intelligent connectivity platform that includes silicon chips, hardware modules and COSMOS software suite. Over the coming quarters, we anticipate ongoing generational product upgrades to existing product lines and introduction of new product categories developed from the ground up to fully utilize the performance and productivity capabilities of generative AI. In summary, over the past few years, we have built a great team that is delivering technology that is foundational to deploying AI infrastructure at scale. We have gained the trust and support of our world class customer base by executing, innovating and delivering to our commitments. These tight relationships are resulting in new product developments and enhanced technology roadmap for Astera. We look forward to continue collaboration with our partners as a new era unfolds driven by AI applications. With that, I will turn the call over to our CFO, Mike Tate, who will discuss our Q1 financial results and Q2 outlook.\nMike Tate: Thanks, Sanjay, and thanks to everyone for joining. This overview of our Q1 financial results and Q2 guidance will be on a non-GAAP basis. The primary difference in Astera Labs non-GAAP metrics is stock-based compensation and the related income tax effects. Please refer to today's press release available on the Investor Relations section of our website for more details on both our GAAP and non-GAAP Q2 financial outlook as well as a reconciliation of our GAAP to non-GAAP financial measures presented on this call. For Q1 of 2024, Astera Labs delivered record quarterly revenue of $65.3 million which was up 29% versus the previous quarter and 269% higher than the revenue in Q1 of 2023. During the quarter, we shipped products to all the major hyperscalers and AI accelerator manufacturers. We recognized revenues across all three of our product families during the quarter with Aries products being the largest contributor. Aries enjoyed solid momentum in AI based platforms as customers continue to introduce and ramp their PCIe Gen 5 capable AI systems, along with overall strong unit growth with the industry's growing investment in generative AI. Also, we continue to make good progress with our Taurus and Leo product lines, which are in the early stages of revenue contribution. In Q1, Taurus revenues were primarily shipping into 200 gig Ethernet based systems, and we expect Taurus revenues to sequentially track higher as we progress through 2024, as we also begin to ship into 400 gig Ethernet based systems. Q1 Leo revenues were largely from customers purchasing pre-products volumes for their development of their next generation CXL capable compute platforms expected to launch late this year with the next server CPU refresh cycle. Q1 non-GAAP gross margins was 78.2% and was up 90 basis points compared with 77.3% in Q4 2023. The positive gross margin performance during the quarter was driven by healthy product mix. Non-GAAP operating expenses for Q1 were $35.2 million up from $27 million in the previous quarter. With non-GAAP operating expenses, R&D expense was $22.9 million, sales and marketing expense was $6 million and general and administration expenses were $6.3 million. Non-GAAP operating expenses during Q1 increased largely due to a combination of increased headcount and incremental costs associated with being a public company. The largest delta between non-GAAP and GAAP operating expenses in Q1 was stock-based compensation recognized in connection with our recent IPO and its associated employer payroll taxes and to a lesser extent our normal quarterly stock-based compensation expense. Non-GAAP operating margins for Q1 was 24.3% as revenues scaled in proportion with our operating expenses on a sequential basis. Interest income in Q1 was $2.6 million. Our non-GAAP tax provision was $4.1 million for the quarter, which represents a tax rate of 22% on a non-GAAP basis. Pro forma non-GAAP fully diluted share count for Q1 was 147.5 million shares. Our pro forma non-GAAP diluted earnings per share for the quarter was $0.10. The pro forma non-GAAP diluted shares includes the assumed conversion of our preferred stock for the entire quarter, while our GAAP share count only includes a conversion of our preferred stock for the step period following our March IPO. Going forward, given that all the preferred stock has now been converted to common stock upon our IPO, those preferred shares will be fully included in the share count for both GAAP and non-GAAP. Cash flow from operating activities for Q1 was $3.7 million and we ended the quarter with cash, cash equivalents and marketable securities of just over $800 million. Now turning to our guidance for Q2 of fiscal 2024. We expect Q2 revenues to increase from Q1 levels within a range of 10% to 12% sequentially. We believe our Aries product family will continue to be the largest component of revenue and will be the primary driver of sequential growth in Q2. Within the Aries product family, we expect the growth to be driven by increased unit demand for AI servers as well as the ramp of new product designs with our customers. We expect non-GAAP gross margins to be approximately 77% given a modest increase in hardware shipments relative to standalone ICs. We believe as our hardware solutions grow as a percentage of revenue over the coming quarters, our gross margins will begin to trend towards our long-term gross margin model of 70%. We expect non-GAAP operating expenses to be approximately $40 million as we remain aggressive in expanding our R&D resource pool across headcount and intellectual property, while also scaling our back office functions. Interest income is expected to be $9 million. Our non-GAAP tax rate should be approximately 23% and our non-GAAP fully diluted share count is expected to be approximately 180 million shares. Adding this all up, we are expecting non-GAAP fully diluted earnings per share of approximately $0.11. This concludes our prepared remarks. Once again, we very much appreciate everyone joining the call. And now we'll open the line for questions. Operator?\nOperator: [Operator Instructions] Our first question will come from the line of Harlan Sur with JPMorgan.\nHarlan Sur : Good afternoon and congratulations on the strong results and guidance post your Q1 as a public company. As you guys mentioned, many new AI XPU programs coming to the market, GPU, ASIC AI chip programs, accelerators. In terms of total XPU shipments this year, I think only half is going to be NVIDIA based, so it is starting to broaden out. The good news is, obviously, the Astera team has exposure to all of these XPU programs. It does seem that the pace of deploying these XPU platforms has accelerated even over the past few months. So how much of the strong results and guidance is due to this acceleration, broadening in customer deployments? How much is more just kind of higher content of Retimers versus your prior expectations? And then do you guys see the strong momentum continuing to the second half of this year?\nMike Tate: Thanks, Harlan. This is Mike. We started shipping into AI servers really in Q3 of last year, so it's just in the early innings. Lot of our customers have not fully deployed their AI system. So we're seeing incremental growth just from adding on the different platforms that we have design wins in. But it's on a, in a backdrop where there's clearly growing investment in AIs as well as overall unit growth is also playing out. As we look out to the balance of this year, there's still a lot of programs that have not ramped yet. So we have the highest confidence that the Gen 5 Aries platform has a lot of growth ahead of it, and that continues into 2025 as well.\nHarlan Sur : And as you mentioned, there's been a lot of focus on next gen PCIe Gen 6 platforms, right, obviously, with the rollout of NVIDIA's Blackwell based platform? And, obviously, with any market that is viewed of as fast growing, you are going to attract competitors. We have seen some announcing by competitors. We know most of the Gen 5 design wins have already been locked up by the Astera team. You've been working with customers, as you mentioned, on Gen 6, for some time now. Maybe how do you compare the customer engagement momentum on Gen 6 versus the same period back when you were working with customers on Gen 5?\nSanjay Gajendra: Good question, Harlan. This is Sanjay here. Let me take that. So like you correctly said, Gen 5 is still a lot of legs on it. Let's be very clear on that. Like Mike noted, we do have platforms that are still ramping and still to come. So to that standpoint, we do expect Gen 5 to be with us for some time. And in terms of Gen 6, again, it's driven by the pace of innovation that's happening on the AI side. There is, as you probably know, there's GPUs are not fully utilized. Some reports put it at around 50%. So there's still a lot of growth in terms of connectivity, which is essentially holding it back, right, meaning there's a pace and a need to adopt faster speeds and links. So, with NVIDIA announcing their Blackwell platform, those are the first set of GPUs that have Gen 6 on that. So for that standpoint, we do expect some of those deployments to happen in 2025. But in general, others are not far behind based upon public information that's out there. So, we do expect the cycle time for Gen 6 adoption to perhaps be a little bit shorter than Gen 5, especially on the AI, server application, more so than the general purpose compute, which is still going to be lagging when it comes to PCIe Gen 6 adoption.\nOperator: Your next question will come from the line of Joe Moore with Morgan Stanley.\nJoe Moore : Following on from that, can you talk about PCI Gen 5 in general purpose servers? It seems like if I look at the CPU penetration of Gen 5, we're still at a pretty early stage. Do you see growth from general purpose and what are the applications driving that?\nSanjay Gajendra: Absolutely. And primarily on the general purpose compute, the main places where the PCIe timer gets used tends to be on the storage connectivity where you have SSDs that are on the back of the server. So to that standpoint, again, it's, there are two things that have been holding it back or three things perhaps. One is just the focus on AI. I mean, most of dollars are going to the AI server application compared to general compute. The second thing is just the ecosystem readiness for Gen 5, primarily on the SSD side, which is starting to evolve with many of the major SSD NVMe players providing or ramping up on Gen 5 based, NVMe drives. The third one really has been the CPU platforms. If you think about it both from Intel and AMD, they're all on the cusp of introducing their next significant platform, whether it is Granite Rapids for Intel or Turin from AMD. So that is expected to drive the introduction of new platform. And if you combine that with the SSDs being ready for Gen 5 and based on the design wins that we already have, you can expect that those things would be a contributing factor as dollars start flowing back into the compute side, general purpose compute side.\nJoe Moore : And for my follow-up, you just mentioned Granite Rapids and Turin, which are the first kind of volume platform supporting CXL 2. What are you hearing in terms of the CPUs will be out, but what will be the initial adoption and how quickly do you think that technology can roll out in 2025?\nSanjay Gajendra: Yes. Let me start off by saying, CXL, every hyperscaler is in some shape or form evaluating and working with the technology. So it's well and alive. I think where the focus really has been in terms of CXL is on the memory expansion use case, specifically for CPUs. And the expansion could be for reasons like adding more memory for large database applications, more capacity memory. And the second use case, of course, is for more memory bandwidth, which are for HPC type of applications. So the thing that's been holding back is the availability of CPUs that support CXL at a production quality level. That will change with Granite Rapids and Turin being available. So at this point, what we can say is that we've been providing chips for quite some time. We've been in preproduction and supported the various different evaluation POC type of activities that have happened with our hyperscaler customers. So, to that standpoint, we do expect revenue to start coming in 2025 from memory expansion use case for CXL.\nOperator: Your next question will come from the line of Tore Svanberg with Stifel.\nTore Svanberg: Yes. Thank you. And let me add my congratulations. My first question is on Gen 6 PCI. So Sanjay, you just mentioned that the design in cycle is going to be shorter than Gen 5 now. Since its backwards compatible for your Gen 5 and especially given the COSMOS software platform, should we assume that you will basically retain most of those sockets that you already had in Gen 5 and then obviously some new ones as well for Gen 6?\nSanjay Gajendra: That's the goal for the company. We have the COSMOS software and like I noted, PCI Express is one of those protocols which, unlike Ethernet, tends to be a little messy, meaning it's something that's been around for a long time. It's a great technology, but it also requires a lot of handholding. And for us, what has happened is being in the customers' platforms, bringing up systems that ramp up to millions of devices has allowed us to understand what are the nuances, what works, what doesn't work, how do you make the link perform at the highest rate. So that tribal knowledge is something that we've captured within the COSMOS software that we built running both on our chips as well as customers' platforms. So we do expect that as Gen 6 starts to materialize, lot of those learnings will be carried over. Now you're right that there's been a lot of competition that has come in as well. But we believe that when it comes to competition, they could have a similar product like us. But no matter what, there is a full time that's essential when it comes to connectivity type of chips, just given the interoperation and getting the kings out and so on. Meaning you could have a perfect chip yet have a failing system. The reason for that is the complexity of the system and how PCI Express standard is defined. So to that standpoint, I agree with what you said in the sense that we have the leading position now in the Retimer market for PCIe and we expect to build on that both with the new features we have added in PCIe Gen 6 or the AEC product line and also the tribal knowledge that we have built by working with our partners over the last three, four years.\nTore Svanberg: That's a great perspective. And as my follow-up, I had a question on AEC. It sounds like that business is going to start ramping late this year. First of all, is that with multiple cable partners? And then related to that, are you the only company today that have, an AEC at 7 meters?\nSanjay Gajendra: I don't know about the only, customer. I would probably request maybe you need to do some research on it on where the competition is. But from a Retimer standpoint, which goes on this, we do have a leading position. So based on that, I would imagine that we are the main provider here, both based on that and the customer traction that we're seeing. So, this one is an interesting use case. So far, PCI Express, as you know, was defined to be inside the server. But what is happening now, and this is why we're excited about PCIe, AECs, is that now we are opening up a new front in terms of clustering GPUs, meaning interconnecting accelerators. That is where the AECs will play, and that is a new opportunity that goes along with the Ethernet AECs that we already provide, which are also used for interconnecting GPUs on the backend network. So, overall, we do believe that combining our PCIe AEC solution and Ethernet AEC solution, we're well set for some of these evolving trends. And our revenue we expect to start coming in for the latter half of this year. And on PCIe, again, we do believe we are the only one just to make sure I clarify what I initially said, just that I don't know if there is someone else talking about it that's not yet in the public domain.\nOperator: Your next question will come from the line of Blayne Curtis with Jefferies.\nBlayne Curtis : Maybe first one for you, Jitendra. Just curious, you mentioned the right architectures, I think, Harlan asked on it. I was just kind of curious about obviously, you have a lead customer and it's a lot of CPU to GPU connections. That's the nature of the market who has the volume. But I'm curious you mentioned back, the backend fabrics a bunch. Kind of curious is that still conceptual? Are you seeing designs for it? And maybe just talk about the widening out of just applications for what the Retimers are being used for?\nJitendra Mohan: Great question. So, there are many applications where we use the Retimers. Of course, we are most known for the connectivity from the GPU to the head node. That is where a lot of the deployments are happening. But these new applications also speak to how rapidly the AI systems are evolving. Every few months, we see a new AI platform come up and that opens up additional opportunities for us. And one of those is to cluster GPUs together. There are two main protocols that are used in addition to NVLink, of course, which are used to cluster GPUs that is PCI Express and Ethernet. And as Sanjay just mentioned, we now have solutions available to interconnect GPUs together, whether they are for PCI Express and/or Ethernet. Specifically, in the case of PCI Express, some of our customers who want to use PCI Express for clustering GPUs together are now able to do so using our PCI Express Retimers, which are offered in the form of an active electrical cable. So this business is going to be in addition to the sustaining business that we have today in connecting GPUs to head nodes. Now we are connecting GPUs together in a cluster. And as you know, these are very intense, very dense mesh connections. So they can grow very, very rapidly. So we are very excited about where this will grow and starting with some revenue contributions later this year.\nBlayne Curtis : And then maybe a question for Mike. The gross margin remained quite high. You said it was mix. I mean, maybe you're just being kind of conservative with the IPO, but I was just kind of curious did the mix come in? I mean, I think it's mostly Retimers. I know as the other products start to ramp that will be the headwind. So, I'm just kind of, how do you think about the rest of the year? Should we kind of just have it kind of come down with mix gradually as those new products ramp off this 70% that you're guiding to?\nMike Tate: Yes. So just to remind everybody, our standalone ICs carry a pretty high margin relative to our hardware solutions. So when the mix gets a little more balanced with hardware versus standalone ICs, we're expecting our long-term gross margins to 10% to 70%. In Q1, we were heavily weighted to standalone ICs, very favorable mix and that's how we enjoyed the strong gross margins. As we go through the balance of this year and into next year, we will see an increasing mix of our modules and also add in cards for CXL as well. So, we think we'll have a gradual trend down towards a long-term model over time as that mix changes.\nOperator: Your next question will come from the line of Thomas O'Malley with Barclays.\nThomas O'Malley : Mike, I just wanted to ask, I know you may not be giving segment details specifically, but could you talk about what you're able to, what contributed to the revenue in the quarter? And then looking out into June, could you talk about from a revenue mix perspective, maybe some sequential help on what's growing? Obviously, the non-ICs business is growing just given the fact that gross margins are pressured a bit, but just any color on the segments would be helpful to start?\nMike Tate: Sure. So as I mentioned, we started shipping into AI server platforms in volume in Q3 and a lot of our customers are still in the ramp mode to the extent we've been shipping for the past couple of quarters. But there's still a lot of designs that haven't even begun to ramp. So, we're still in the early phases that if you look out in time, we see the Gen 5 piece of it in AI continue to grow into next year as well. So as you look into Q2, the growth that we're guiding to is still largely driven by the Aries Gen 5 deployment in AI servers both for existing platforms with increased unit volumes, but also the new customers begin their ramps as well.\nThomas O'Malley : And then just a broader one. In talking with NVIDIA, they're referencing your GP-200 architecture becoming a bigger percent of the mix, NVLink 72 being more of the deployments that hyperscalers are taking. When you look at the Hopper architecture versus the Blackwell architecture and their NV72 platform, where they're using NVLink amongst their GPUs, can you talk about the puts and takes when it comes to your retiming product? Do you see an attach rate that's any different than the current generation?\nJitendra Mohan: Let me take that. Great question. First, let me say that we are just at the beginning phases of AI. We will continue to see new architectures being produced by AI platform providers at a very rapid pace, just match up with the growth in AI models. And on top of that, we'll see innovative ways that hyperscalers will deploy these platforms in their cloud. So as these architectures evolve, so do the connectivity challenges. Some challenges are going to be incremental and some are going to be completely new. And so what we believe is given the increasing speeds, increasing complexities with these new platforms, we do expect our dollar content per AS platform to increase over time. We see these developments providing us good tailwinds going here into the future. So now to your question about the GP-200 specifically, well, first of all, we cannot speak about specific customer architectures. But here is something that is very clear to see. As the AI platform providers produce these new architectures, the hyperscalers will choose different form factors to deploy them. And in that way, no two clouds are the same. Each hyperscaler has a unique requirement, unique constraint to deploy these AI platforms and we are working with all of them to enable these deployments. This combination of new platforms and very cloud specific deployment strategies, it presents great opportunities for our PCIe connectivity portfolio. And to that point, as Sanjay mentioned, we announced the sampling of our Gen 6 Retimer during GTC. If you look at our press release, you will see that broad support from AI platform providers. And to this day, to the best of our knowledge, we are still the only one sampling agnostic solution. So, on the whole, given the fact that speeds are increasing, complexity is increasing, in fact the pace of innovation is going up as well, these all play to our strengths and we have customers coming to us for new approaches to solve these problems. So we feel very good about the potential to grow our PCIe connectivity business.\nOperator: Your next question will come from the line of Quinn Bolton with Needham.\nQuinn Bolton : I just wanted to follow-up on the use of PCI Express in the GPU to GPU backend networks. I think that's something historically you had excluded from your TAM, but it looks like it's becoming an opportunity here and starts to ramp in the second half of this year. Wondering if you could just talk about the breadth of some of the custom AI accelerators that are choosing PCI Express as their interconnect over, say, Ethernet? And then I've got a follow-up.\nJitendra Mohan: Again, great question. So just to kind of follow-up the response that we provided before. There are two, three dominant protocols that are used to cluster GPUs together. The one that's most well known, of course, is NVLink, which is what NVIDIA uses and is the proprietary interface. The other two are Ethernet and PCI Express. We do see some of our customers using PCI Express and I think it's not appropriate to say who, but certainly PCI Express is a fairly common protocol. It is the one that's natively found on all GPUs and CPUs and then others data center components. Ethernet is also very popular and to the extent that a particular customer chooses to use Ethernet or PCI Express, we are able to support them both with our solutions, the Aries PCIe Retimer family as well as the Taurus Ethernet Retimer family. We do expect these two to make meaningful contributions to our revenue, as I mentioned starting with the end of this year and then of course continuing into next year.\nQuinn Bolton : And my second question is you guys have talked about introduction of new products as new TAM expansion activity and I'm not going to ask you to introduce them today. But just in terms of timing as we think out, are these new products timeline sort of introduction later this year or 2025 with revenue ramp in 2026? Is that the general framework investors should be thinking about the new products that you've discussed?\nSanjay Gajendra: Again, I think we, as a company don't talk about unreleased products, the timing of it. But what I can share with you is the following. First, we've been very fortunate to be in the central seat of AI deployment and enjoy a great relationship with the hyperscalers and AI platform providers. So we get to see a lot, we get to hear a lot in terms of some of the requirements. So clearly, we are going to be developing products that address the bottlenecks, whether it is on the data side, network side or on the memory side. So we are working on several products, as you can imagine, that would all be developed ground up for AI infrastructure and enable connectivity solutions that will deploy the AI application sooner. There is a lot going on, a lot of new infrastructure, a lot of new GPU announcements, CPU announcement. So, you can imagine given the pace of this market and the changes that are upcoming, we do anticipate that this will all start having meaningful impact and incremental revenue impact to our business.\nOperator: Your next question will come from the line of Ross Seymore with Deutsche Bank.\nRoss Seymore : I wanted to go into the ASIC versus GPU side of things. As ASIC start to penetrate this market to certain degrees, how does that change, if any, the Retimer, TAM that you would have? And I guess even the competitive dynamic in that equation considering one of the biggest ASICs suppliers is also an aspiring competitor of yours?\nJitendra Mohan: So, great question again. Let me just refer back to what I said, which is we will see more and more different solutions come to the market to address the evolving AI requirements. Some of them are going to be GPUs from the kind of known AI providers like NVIDIA, AMD and others. And some others will be custom built ASICs that are built typically by hyperscalers, whether they are AWS or Microsoft or Google and others. And the requirements for these two systems are common in some ways, but they do differ. For example, what particular type of backend connectivity they use and exactly what are the ins and outs that are going to each of these chips. The good news is with the breadth of portfolio that we have and the close engagement with the several ASIC providers as well as the GPU providers, we understand the challenges of these systems very well. And not only are we providing solutions that address those today with current generation, we are engaged with them very closely on the next generation, on the upcoming platforms, whether they are GPU based or ASIC based to provide these solutions. Great example was the Aries SCM, where we enabled using our trusted solution for PCI Express Retimers. We enabled a new way of connecting some of these ASICs on the backend network.\nSanjay Gajendra: And just maybe if I can add to that, one way to visualize connectivity market or subsystem is the nervous system within the human anatomy. Right? It's one of those things where you don't want to mess with it. Yes. There will be a sick vendor. There are options or off-the-shelf. Once the nervous system is built, tested, especially like what we have developed where the nervous system that we've built is specifically done for AI application. And there's a lot of qualification, a lot of software investment that hyperscalers have done. And they want to reuse that across different kinds of topologies, whether it's ASIC based or merchant silicon based. And we do see a trend happening, when we look at customers that we're engaged with today. And for protocols like PCI Express, Ethernet, and CXL, and especially where as Taurus plays, these are standards based. So to that standpoint, whatever end possible architecture is being used, we believe that we will stand to gain from that.\nRoss Seymore : I guess as my follow-up one quick one for Mike, how should we think about OpEx beyond the second quarter? I know there's a bigger step up there with a full quarter of being a publicly traded company etcetera, but just walk us through your OpEx plans rest of the year or even to the target?\nMike Tate: Yes, I mean, thanks Ross. We are continuing to try to invest quite a bit in headcount particularly in R&D. There's so many opportunities ahead of us that we love to get a jump on those products and also improve the time to market. That being said, we're pretty selective on who we bring into the company. So that will just meter our growth. And we believe our OpEx although it's going to be increasing will probably not increase at a rate of revenue over the near- and long-term. And that's why we feel good about a long-term operating margin model of 40%. So over time, we feel confident we can trend that direction even with increasing investment in OpEx.\nOperator: Your next question will come from the line of Suji Desilva with ROTH MKM.\nSuji Desilva : Hi Jitendra, Sanjay, Mike congrats on the first quarter here. On the backend, the addressable market here that's non NVLink, I'm trying to understand if the PCIe and Ethernet opportunities there will be adopted at a similar pace out of the gate or whether PCI would lead that adoption, in the non NVLink backend opportunity?\nSanjay Gajendra: It's hard to say at this point just because there is so much of development going on here. I mean, you can imagine the non NVIDIA ecosystem, they will rely on standards technologies, whether it is PCI Express or Ethernet. And the advantage of PCI Express is that it's low latency. Right? Significantly low latency compared to Ethernet. So there are some benefits to that. And there are certain extensions that people consider to add on top of PCI Express, or when it comes to the proprietary implementation. So, overall, we do see this, from a technology standpoint, PCI Express will have that advantage. Now Ethernet also has been around, so we'll have to wait and see how all of this develops over the next, let's say, 6 to 18 months.\nJitendra Mohan: Yes. Add to what Sanjay said. I think the good news for us in some ways is that we don't have to pick, we don't have to decide which one. We have chips, we have hardware, and we have software. So we have customers that come to us and say, hey, I need this for my new AI platform. Can you help me with that? And then that's what we've been doing.\nSuji Desilva : And then a question perhaps for Mike. The initial AEC programs ramping maybe a few customers this year, few customers next year or maybe perhaps all of them this year. But do you perceive that those will be larger lumpier program based ramps, Mike? Or will those be steady kind of build out to the servers grow?\nMike Tate: I think the product ramps will mirror other product ramps well. They'll gradually build over a few quarters till they hit steady state. And if you layer them on top of each other it just continues to build a nice growing revenue profile. So as you look at Taurus in 2024, we're shipping 200 gig right now. And then in the back half we start to ship 400 gig. And if you look into 2025, 800 gig which is ultimately the biggest opportunity in a much broader set of customers, will be when the market really becomes very large.\nOperator: Your next question will come from the line of Richard Shannon with Craig-Hallum.\nRichard Shannon : Hi, guys. Thanks for taking my questions. Well, congratulations on coming public here. I guess I want to follow-up on a couple of topics here that have been hit on, including Suji's question here about the PCI Express, AEC opportunity. Are these design wins, or are these kind of pre-design win ramps you're talking about this year? And I guess ultimately my question on this topic here is, is can this opportunity, these PCI Express AECs become as big as your Taurus family in the foreseeable future?\nSanjay Gajendra: Yes. So these are design wins to clarify. We have been shipping this. We announced this. We've demonstrated this at, at public forums. So to that standpoint, it's an opportunity that we're excited about and like we noted early on. We expect to start contributing revenue for later half of this year.\nRichard Shannon : And the second question is on CXL. I think you've mentioned a couple of applications here. Maybe if you can kind of express the breadth of interest here across hyperscalers and other customers, for the ones you mentioned? And then also for the, next ones that are a little bit more expansive in nature, how do you see the testing and specking out of those? Are those coming to market at the time you're hoping for or is there a little bit more development required to get those to market?\nSanjay Gajendra: Yes. There are two questions. Let me take the first one, which is the CXL side. For CXL, there are four main use cases to keep in mind, memory expansion, memory tiering, where you're trying to go for a TCO type of angle, memory pooling, and what is called as this memory drives that Samsung and others are providing. We believe memory drives are more suitable for the enterprise customers. And whereas the first three are more suitable for cloud scale deployment. And there, again, memory pulling is something that's further out in time is our belief just because it requires software changes. So the ones that are more sort of short-term, medium-term is memory expansion and memory tiering. And like I noted early on, all the major hyperscalers, at least in the U.S., are all engaged on the CXL technology. But it is going to be a matter of time with both CPUs being available and dollars being available from a general purpose compute standpoint. Okay. And then in terms of, your second question was, was that more on new products? Was that the context for it?\nRichard Shannon : Yes.\nSanjay Gajendra: Yes. So again, we don't talk about the exact time frame, but you can imagine our last product we announced was a little over a year ago. So our engineers have not been quite, so they've been working hard. So to that standpoint, we are working very diligently and hard based upon, lot of interest and engagement from customers that we've already been working with.\nOperator: There are no further questions at this time. I'll turn the call back over to Leslie Green for closing remarks.\nLeslie Green: Thank you everyone for your participation and questions. We look forward to seeing many of you at various financial conferences this summer and updating you on our progress on our Q2 earnings conference call. Thank you.\nJitendra Mohan: Thank you, guys.\nOperator: This concludes today's conference call. You may now disconnect.",
        "speaker1": {
            "name": "Mike Tate",
            "content": "Thanks, Sanjay, and thanks to everyone for joining. This overview of our Q1 financial results and Q2 guidance will be on a non-GAAP basis. The primary difference in Astera Labs non-GAAP metrics is stock-based compensation and the related income tax effects. Please refer to today's press release available on the Investor Relations section of our website for more details on both our GAAP and non-GAAP Q2 financial outlook as well as a reconciliation of our GAAP to non-GAAP financial measures presented on this call. For Q1 of 2024, Astera Labs delivered record quarterly revenue of $65.3 million which was up 29% versus the previous quarter and 269% higher than the revenue in Q1 of 2023. During the quarter, we shipped products to all the major hyperscalers and AI accelerator manufacturers. We recognized revenues across all three of our product families during the quarter with Aries products being the largest contributor. Aries enjoyed solid momentum in AI based platforms as customers continue to introduce and ramp their PCIe Gen 5 capable AI systems, along with overall strong unit growth with the industry's growing investment in generative AI. Also, we continue to make good progress with our Taurus and Leo product lines, which are in the early stages of revenue contribution. In Q1, Taurus revenues were primarily shipping into 200 gig Ethernet based systems, and we expect Taurus revenues to sequentially track higher as we progress through 2024, as we also begin to ship into 400 gig Ethernet based systems. Q1 Leo revenues were largely from customers purchasing pre-products volumes for their development of their next generation CXL capable compute platforms expected to launch late this year with the next server CPU refresh cycle. Q1 non-GAAP gross margins was 78.2% and was up 90 basis points compared with 77.3% in Q4 2023. The positive gross margin performance during the quarter was driven by healthy product mix. Non-GAAP operating expenses for Q1 were $35.2 million up from $27 million in the previous quarter. With non-GAAP operating expenses, R&D expense was $22.9 million, sales and marketing expense was $6 million and general and administration expenses were $6.3 million. Non-GAAP operating expenses during Q1 increased largely due to a combination of increased headcount and incremental costs associated with being a public company. The largest delta between non-GAAP and GAAP operating expenses in Q1 was stock-based compensation recognized in connection with our recent IPO and its associated employer payroll taxes and to a lesser extent our normal quarterly stock-based compensation expense. Non-GAAP operating margins for Q1 was 24.3% as revenues scaled in proportion with our operating expenses on a sequential basis. Interest income in Q1 was $2.6 million. Our non-GAAP tax provision was $4.1 million for the quarter, which represents a tax rate of 22% on a non-GAAP basis. Pro forma non-GAAP fully diluted share count for Q1 was 147.5 million shares. Our pro forma non-GAAP diluted earnings per share for the quarter was $0.10. The pro forma non-GAAP diluted shares includes the assumed conversion of our preferred stock for the entire quarter, while our GAAP share count only includes a conversion of our preferred stock for the step period following our March IPO. Going forward, given that all the preferred stock has now been converted to common stock upon our IPO, those preferred shares will be fully included in the share count for both GAAP and non-GAAP. Cash flow from operating activities for Q1 was $3.7 million and we ended the quarter with cash, cash equivalents and marketable securities of just over $800 million. Now turning to our guidance for Q2 of fiscal 2024. We expect Q2 revenues to increase from Q1 levels within a range of 10% to 12% sequentially. We believe our Aries product family will continue to be the largest component of revenue and will be the primary driver of sequential growth in Q2. Within the Aries product family, we expect the growth to be driven by increased unit demand for AI servers as well as the ramp of new product designs with our customers. We expect non-GAAP gross margins to be approximately 77% given a modest increase in hardware shipments relative to standalone ICs. We believe as our hardware solutions grow as a percentage of revenue over the coming quarters, our gross margins will begin to trend towards our long-term gross margin model of 70%. We expect non-GAAP operating expenses to be approximately $40 million as we remain aggressive in expanding our R&D resource pool across headcount and intellectual property, while also scaling our back office functions. Interest income is expected to be $9 million. Our non-GAAP tax rate should be approximately 23% and our non-GAAP fully diluted share count is expected to be approximately 180 million shares. Adding this all up, we are expecting non-GAAP fully diluted earnings per share of approximately $0.11. This concludes our prepared remarks. Once again, we very much appreciate everyone joining the call. And now we'll open the line for questions. Operator? Thanks, Harlan. This is Mike. We started shipping into AI servers really in Q3 of last year, so it's just in the early innings. Lot of our customers have not fully deployed their AI system. So we're seeing incremental growth just from adding on the different platforms that we have design wins in. But it's on a, in a backdrop where there's clearly growing investment in AIs as well as overall unit growth is also playing out. As we look out to the balance of this year, there's still a lot of programs that have not ramped yet. So we have the highest confidence that the Gen 5 Aries platform has a lot of growth ahead of it, and that continues into 2025 as well. Yes. So just to remind everybody, our standalone ICs carry a pretty high margin relative to our hardware solutions. So when the mix gets a little more balanced with hardware versus standalone ICs, we're expecting our long-term gross margins to 10% to 70%. In Q1, we were heavily weighted to standalone ICs, very favorable mix and that's how we enjoyed the strong gross margins. As we go through the balance of this year and into next year, we will see an increasing mix of our modules and also add in cards for CXL as well. So, we think we'll have a gradual trend down towards a long-term model over time as that mix changes. Sure. So as I mentioned, we started shipping into AI server platforms in volume in Q3 and a lot of our customers are still in the ramp mode to the extent we've been shipping for the past couple of quarters. But there's still a lot of designs that haven't even begun to ramp. So, we're still in the early phases that if you look out in time, we see the Gen 5 piece of it in AI continue to grow into next year as well. So as you look into Q2, the growth that we're guiding to is still largely driven by the Aries Gen 5 deployment in AI servers both for existing platforms with increased unit volumes, but also the new customers begin their ramps as well.\nThomas O'Malley:And then just a broader one. In talking with NVIDIA, they're referencing your GP-200 architecture becoming a bigger percent of the mix, NVLink 72 being more of the deployments that hyperscalers are taking. When you look at the Hopper architecture versus the Blackwell architecture and their NV72 platform, where they're using NVLink amongst their GPUs, can you talk about the puts and takes when it comes to your retiming product? Do you see an attach rate that's any different than the current generation? Yes, I mean, thanks Ross. We are continuing to try to invest quite a bit in headcount particularly in R&D. There's so many opportunities ahead of us that we love to get a jump on those products and also improve the time to market. That being said, we're pretty selective on who we bring into the company. So that will just meter our growth. And we believe our OpEx although it's going to be increasing will probably not increase at a rate of revenue over the near- and long-term. And that's why we feel good about a long-term operating margin model of 40%. So over time, we feel confident we can trend that direction even with increasing investment in OpEx. I think the product ramps will mirror other product ramps well. They'll gradually build over a few quarters till they hit steady state. And if you layer them on top of each other it just continues to build a nice growing revenue profile. So as you look at Taurus in 2024, we're shipping 200 gig right now. And then in the back half we start to ship 400 gig. And if you look into 2025, 800 gig which is ultimately the biggest opportunity in a much broader set of customers, will be when the market really becomes very large."
        },
        "speaker2": {
            "name": "Jitendra Mohan",
            "content": "Thank you, Leslie. Good afternoon, everyone, and thanks for joining our first earnings conference call as a public company. This year is off to a great start with Astera Labs seeing strong and continued momentum along with the successful execution of our IPO in March. First and foremost, I would like to thank our investors, customers, partners, suppliers and employees for their steadfast support over the past six years. We have built Astera Labs from the ground up to address the connectivity bottlenecks to unlock the full potential of AI in the cloud. With your help, we've been able to scale the company and deliver innovative technology solutions to the leading hyperscalers and AI platform providers worldwide. But our work is only just beginning. We are supporting the accelerated pace of AI infrastructure deployments with leading hyperscalers by developing new product categories, while also exploring new market segments. Looking at industry reports over the past several weeks, it is clear that we remain in the early stages of a transformative investment cycle by our customers to build out the next generation of infrastructure that is needed to support their AI roadmaps. According to recent earning reports, on a consolidated basis, CapEx spend during the first quarter for the four largest U.S. hyperscalers grew by roughly 45% year-on-year to nearly $50 billion. Qualitative commentary implies continued quarterly growth in CapEx for this group through the balance of the year. This is truly an exciting time for technology innovators within the cloud and AI infrastructure market, and we believe Astera Labs is well position to benefit from these growing investment trends. Against the strong industry backdrop, Astera Labs delivered strong Q1 results with record revenue, strong non-GAAP operating margin, positive operating cash flows, while also introducing two new products. Our revenue in Q1 was $65.3 million up 29% from the previous quarter and up 269% from the same period in 2023. Non-GAAP operating margin was 24.3%, and we delivered $0.10 of pro forma non-GAAP diluted earnings per share. I will now provide some commentary around our position in this rapidly evolving AI market. Then I will turn the call over to Sanjay to discuss new products and our growth strategy. Finally, Mike will provide additional details on our Q1 results and our Q2 financial guidance. Complex AI model sizes continue doubling about every six months, fueling the demand for high performance AI platforms running in the cloud. Modern GPUs and AI accelerators are phenomenally good at compute, but without equally fast connectivity, they remain highly underutilized. Technology innovation within the AI Accelerator market has been moving forward at an incredible pace and the number and variety of architectures continues to expand to handle trillion parameter models, while improving AI infrastructure utilization. We continue to see our hyperscaler customers utilize the latest merchant GPUs and proprietary AI accelerators to compose unique data center scale AI infrastructure. However, no two clouds are the same. The major hyperscalers are architecting their systems to deliver maximum AI performance based on the specific cloud infrastructure requirements, from power and cooling to connectivity. We are working alongside our customers to ensure these complex and different architectures achieve maximum performance and operate reliably even as data rates continue to double. As the systems continue to move data faster and grow in complexity, we expect to see our average dollar content per AI platform increase and even more so with the new products we have in development. Our conviction in maintaining and strengthening our leadership position in the market is rooted in our comprehensive intelligent connectivity platform and our deep customer partnerships. The foundation of our platform consists of semiconductor based and software-defined connectivity ICs, modules and boards, which all support our COSMOS software suite. We provide customers with a complete customizable solution, tips, hardware and software, which maximizes flexibility without performance penalties, delivers deep fleet management capabilities and matches space with the ever quickening product introduction cycles of our customers. Not only does COSMOS software run on our entire product portfolio, but it is also integrated within our customers' operating stacks to deliver seamless customization, optimization and monitoring. Today, Astera Labs is focused on three core technology standards: PCI Express, Ethernet and Compute Express Link. We're shipping three separate product families, all generating revenue and in various stages of adoption and deployment supporting these different connectivity protocols. Let me touch upon each of these critical data center connectivity standards and how we support them with our differentiated solutions. First, PCI Express. PCIe is the native interface on all AI accelerators, TPUs and GPUs, and is the most prevalent protocol for moving data at high bandwidth and low latency inside servers. Today, we see PCIe Gen 5 getting widely deployed in AI servers. These AI servers are becoming increasingly complex. Faster signal speeds in combination with complex server topologies are driving significant signal integrity challenges. To help solve these problems, our hyperscalers and AI accelerator customers utilize our PCIe Smart DSP Retimers to extend the reach of PCIe Gen 5 between various components within heterogeneous compute architecture. Our Aries product family represents the gold standard in the industry for performance, robustness and flexibility, and is the most widely deployed solution in the market today. Our leadership position with millions of critical data links running through our Aries Retimers and our COSMOS software enables us to do something more, become the eyes and ears to monitor the connectivity infrastructure and help fleet managers ensure their AI infrastructure is operating at fleet utilization. Deep diagnostics and monitoring capabilities in our chips and extensive fleet management features in our COSMOS software, which are deployed together in our customer's fleet has become a material differentiator for us. Our COSMOS software provides the easiest and fastest path to deploy the next generation of our devices. We see AI workloads and newer GPUs driving the transition from PCIe Gen 5 running at 32 gigabits per second per lane to PCIe Gen 6 running at 64 gigabits per second per lane. Our customers are evaluating our Gen 6 solutions now, and we expect them to make design decisions in the next six to nine months. In addition, while we see our Aries devices being heavily deployed today for interconnecting AI accelerators with CPUs and networking, we also expect our Aries devices to play an increasing role in backend fabrics, interconnecting AI Accelerators to each other in AI clusters. Next, let's talk about Ethernet. Ethernet protocol is extensively deployed to build large scale networks within data centers. Today, Ethernet makes up the vast majority of connections between servers and top of rack switches. Driven by AI workloads' insatiable need for speed, Ethernet data rates are doubling roughly every two years, and we expect the transition from 400 gig Ethernet to 800 gig Ethernet to take place later in 2025. 800 gig Ethernet is based on 100 gigabits per second per lane signaling rate, which is facing tremendous pressure on conventional passive cabling solutions. Like our PCIe Retimers, our portfolio of Taurus Ethernet Retimers helps relieve these connectivity bottlenecks by overcoming the reach, signal integrity and bandwidth issues by enabling robust 100 gig per lane connectivity over copper. Unlike our Aries portfolio, which is largely sold in a chip format, we sell our Taurus portfolio largely in the form of smart cable modules that are assembled into active electrical cables by our cable partners. This approach allows us to focus on our strength and fully leverage our COSMOS software suite to offer customization, easy qualification, deep telemetry and field upgrade to our customers. At the same time, this model enables our cable partners to continue to excel at bringing the best cabling technology to our common end customers. We expect 400 deployments based on our Taurus smart cable modules to begin to ramp in the back half of 2024. We see the transition to 800 gig Ethernet starting to happen in 2025, resulting in broad demand for AECs to both scale up and scale out AI infrastructure and strong growth for our Taurus Ethernet Smart Cable module portfolio over the coming years. Last is Compute Express Link or CXL. CXL is a low latency cash coherent protocol, which runs on top of PCIe protocol. CXL provides an open standard for disaggregating memory from compute. CXL allows you to balance the memory bandwidth and capacity requirements independently from compute requirements, resulting in better utilization of compute infrastructure. Over the next several years, data center platform architects plan to utilize CXL technology to solve memory bandwidth and capacity bottlenecks that are being exacerbated by the exponential increase in compute capability of CPUs and GPUs. Major hyperscalers are actively exploring different application of CXL memory expansion. While the adoption of CXL technology is currently in its infancy, we do expect to see increased deployments with the introduction of next generation CXL capable datacenter server CPUs such as Granite Rapids, Turing and others. Our first to market portfolio of Leo CXL memory connectivity controllers is very well positioned to enable our customers to overcome memory bottlenecks and deliver significant benefits to their end customers. We have worked closely with our hyperscaler customers and CPU partners to optimize our solution to seamlessly deliver these benefits without any application level software changes. Furthermore, we have used our COSMOS software to include significant learnings we have had over the last 18 months and to customize our Leo memory expansion solution to the different requirements from each hyperscaler. We anticipate memory expansion will be the first high volume use case that will drive design wins into volume production in 2025 timeframe. We remain very excited about the potential of CXL in datacenter applications and believe that most new CPUs will support CXL and hyperscalers will increasingly deploy innovative solutions based on CXL. With that, let me turn the call over to our President and COO, Sanjay Gajendra, to discuss some of our recent product announcements and our long-term growth strategy. Great question. So, there are many applications where we use the Retimers. Of course, we are most known for the connectivity from the GPU to the head node. That is where a lot of the deployments are happening. But these new applications also speak to how rapidly the AI systems are evolving. Every few months, we see a new AI platform come up and that opens up additional opportunities for us. And one of those is to cluster GPUs together. There are two main protocols that are used in addition to NVLink, of course, which are used to cluster GPUs that is PCI Express and Ethernet. And as Sanjay just mentioned, we now have solutions available to interconnect GPUs together, whether they are for PCI Express and/or Ethernet. Specifically, in the case of PCI Express, some of our customers who want to use PCI Express for clustering GPUs together are now able to do so using our PCI Express Retimers, which are offered in the form of an active electrical cable. So this business is going to be in addition to the sustaining business that we have today in connecting GPUs to head nodes. Now we are connecting GPUs together in a cluster. And as you know, these are very intense, very dense mesh connections. So they can grow very, very rapidly. So we are very excited about where this will grow and starting with some revenue contributions later this year. Let me take that. Great question. First, let me say that we are just at the beginning phases of AI. We will continue to see new architectures being produced by AI platform providers at a very rapid pace, just match up with the growth in AI models. And on top of that, we'll see innovative ways that hyperscalers will deploy these platforms in their cloud. So as these architectures evolve, so do the connectivity challenges. Some challenges are going to be incremental and some are going to be completely new. And so what we believe is given the increasing speeds, increasing complexities with these new platforms, we do expect our dollar content per AS platform to increase over time. We see these developments providing us good tailwinds going here into the future. So now to your question about the GP-200 specifically, well, first of all, we cannot speak about specific customer architectures. But here is something that is very clear to see. As the AI platform providers produce these new architectures, the hyperscalers will choose different form factors to deploy them. And in that way, no two clouds are the same. Each hyperscaler has a unique requirement, unique constraint to deploy these AI platforms and we are working with all of them to enable these deployments. This combination of new platforms and very cloud specific deployment strategies, it presents great opportunities for our PCIe connectivity portfolio. And to that point, as Sanjay mentioned, we announced the sampling of our Gen 6 Retimer during GTC. If you look at our press release, you will see that broad support from AI platform providers. And to this day, to the best of our knowledge, we are still the only one sampling agnostic solution. So, on the whole, given the fact that speeds are increasing, complexity is increasing, in fact the pace of innovation is going up as well, these all play to our strengths and we have customers coming to us for new approaches to solve these problems. So we feel very good about the potential to grow our PCIe connectivity business. Again, great question. So just to kind of follow-up the response that we provided before. There are two, three dominant protocols that are used to cluster GPUs together. The one that's most well known, of course, is NVLink, which is what NVIDIA uses and is the proprietary interface. The other two are Ethernet and PCI Express. We do see some of our customers using PCI Express and I think it's not appropriate to say who, but certainly PCI Express is a fairly common protocol. It is the one that's natively found on all GPUs and CPUs and then others data center components. Ethernet is also very popular and to the extent that a particular customer chooses to use Ethernet or PCI Express, we are able to support them both with our solutions, the Aries PCIe Retimer family as well as the Taurus Ethernet Retimer family. We do expect these two to make meaningful contributions to our revenue, as I mentioned starting with the end of this year and then of course continuing into next year. So, great question again. Let me just refer back to what I said, which is we will see more and more different solutions come to the market to address the evolving AI requirements. Some of them are going to be GPUs from the kind of known AI providers like NVIDIA, AMD and others. And some others will be custom built ASICs that are built typically by hyperscalers, whether they are AWS or Microsoft or Google and others. And the requirements for these two systems are common in some ways, but they do differ. For example, what particular type of backend connectivity they use and exactly what are the ins and outs that are going to each of these chips. The good news is with the breadth of portfolio that we have and the close engagement with the several ASIC providers as well as the GPU providers, we understand the challenges of these systems very well. And not only are we providing solutions that address those today with current generation, we are engaged with them very closely on the next generation, on the upcoming platforms, whether they are GPU based or ASIC based to provide these solutions. Great example was the Aries SCM, where we enabled using our trusted solution for PCI Express Retimers. We enabled a new way of connecting some of these ASICs on the backend network. Yes. Add to what Sanjay said. I think the good news for us in some ways is that we don't have to pick, we don't have to decide which one. We have chips, we have hardware, and we have software. So we have customers that come to us and say, hey, I need this for my new AI platform. Can you help me with that? And then that's what we've been doing. Thank you, guys."
        },
        "speaker3": {
            "name": "Sanjay Gajendra",
            "content": "Thanks, Jitendra, and good afternoon, everyone. Astera Labs is well positioned to demonstrate long-term growth through a combination of three factors. One, we have a strong secular tailwinds with increased AI infrastructure investment. Two, the next generation of products within existing product lines are gaining traction. And third, the introduction of new product lines. Over the past three months, we announced two new and significant products that play an important role in enabling next generation AI platforms and provide incremental revenue opportunities as early as the second half of 2024. First, we expanded our widely deployed field proven Aries Smart DSP Retimers portfolio with the introduction and public demonstration of our Aries 6 PCIe Retimer that delivers robust, low power PCIe Gen 6 and CXL 3 connectivity between next generation GPUs, AI accelerators, CPUs, NICs, and CXL memory controllers. Aries 6 is the third generation of our PCIe Smart Retimer portfolio and provides the bandwidth required to support data intensive AI workloads while maximizing utilization of next generation GPUs operating at 64 gigabit per second per link. Fully compatible with our field deployed COSMOS software suite, Aries 6 incorporates the tribal knowledge we have acquired over the past four years by partnering and enabling hyperscadeless to deploy AI infrastructure in the cloud. Aries 6 also enables the seamless upgrade path from current PCIe Gen 5 based platforms to next generation PCIe Gen 6 based platforms for our customers. With Aries 6, we demonstrated industry's lowest power at 11 watts at Gen 6 in full 16 lane configuration running at 64 gigabit per second per lane, significantly lower than our competitors and even lower than our own Aries Gen 5 Retimer. Through collaboration with leading providers of GPUs and CPUs such as AMD, ARM, Intel, and NVIDIA, Aries 6 is being rigorously tested at Astera's Cloud-Scale Interop Lab and in customers' platforms to minimize interoperation risk, lower system development cost, and reduce time to market. Aries 6 was demonstrated at NVIDIA's GTC event during the week of March 18th. Aries 6 is currently sampling two leading AI and cloud infrastructure providers, and we expect initial volume ramps to begin in 2025. We also announced the introduction and sampling of our Aries PCIe and CXL Smart Cable Modules for Active Electrical Cables or AECs to support robust and long reach, up to 7 meters copper cable connectivity. This is 3x the standard reach defined in the PCIe spec. Our new PCIe AEC solution is design for GPU clustering application by extending PCIe backend fabric deployments to multiple racks. This new Aries product category expands our market opportunity from within the rack to across racks. As with our entire product portfolio, Aries Smart Cable Modules support our COSMOS software suite to deliver a powerful yet familiar array of link monitoring, fleet management and rack tools which are customizable for diverse needs of our hyperscaler customers. We leveraged our expertise in silicon, hardware and software to deliver a complete solution in record time and we expect initial shipments to begin later this year for the PCIe AECs. We believe this new Aries product announcement represents another concrete example of Astera Labs driving the PCIe ecosystem with technology leadership with an intelligent connectivity platform that includes silicon chips, hardware modules and COSMOS software suite. Over the coming quarters, we anticipate ongoing generational product upgrades to existing product lines and introduction of new product categories developed from the ground up to fully utilize the performance and productivity capabilities of generative AI. In summary, over the past few years, we have built a great team that is delivering technology that is foundational to deploying AI infrastructure at scale. We have gained the trust and support of our world class customer base by executing, innovating and delivering to our commitments. These tight relationships are resulting in new product developments and enhanced technology roadmap for Astera. We look forward to continue collaboration with our partners as a new era unfolds driven by AI applications. With that, I will turn the call over to our CFO, Mike Tate, who will discuss our Q1 financial results and Q2 outlook. Good question, Harlan. This is Sanjay here. Let me take that. So like you correctly said, Gen 5 is still a lot of legs on it. Let's be very clear on that. Like Mike noted, we do have platforms that are still ramping and still to come. So to that standpoint, we do expect Gen 5 to be with us for some time. And in terms of Gen 6, again, it's driven by the pace of innovation that's happening on the AI side. There is, as you probably know, there's GPUs are not fully utilized. Some reports put it at around 50%. So there's still a lot of growth in terms of connectivity, which is essentially holding it back, right, meaning there's a pace and a need to adopt faster speeds and links. So, with NVIDIA announcing their Blackwell platform, those are the first set of GPUs that have Gen 6 on that. So for that standpoint, we do expect some of those deployments to happen in 2025. But in general, others are not far behind based upon public information that's out there. So, we do expect the cycle time for Gen 6 adoption to perhaps be a little bit shorter than Gen 5, especially on the AI, server application, more so than the general purpose compute, which is still going to be lagging when it comes to PCIe Gen 6 adoption. Absolutely. And primarily on the general purpose compute, the main places where the PCIe timer gets used tends to be on the storage connectivity where you have SSDs that are on the back of the server. So to that standpoint, again, it's, there are two things that have been holding it back or three things perhaps. One is just the focus on AI. I mean, most of dollars are going to the AI server application compared to general compute. The second thing is just the ecosystem readiness for Gen 5, primarily on the SSD side, which is starting to evolve with many of the major SSD NVMe players providing or ramping up on Gen 5 based, NVMe drives. The third one really has been the CPU platforms. If you think about it both from Intel and AMD, they're all on the cusp of introducing their next significant platform, whether it is Granite Rapids for Intel or Turin from AMD. So that is expected to drive the introduction of new platform. And if you combine that with the SSDs being ready for Gen 5 and based on the design wins that we already have, you can expect that those things would be a contributing factor as dollars start flowing back into the compute side, general purpose compute side. Yes. Let me start off by saying, CXL, every hyperscaler is in some shape or form evaluating and working with the technology. So it's well and alive. I think where the focus really has been in terms of CXL is on the memory expansion use case, specifically for CPUs. And the expansion could be for reasons like adding more memory for large database applications, more capacity memory. And the second use case, of course, is for more memory bandwidth, which are for HPC type of applications. So the thing that's been holding back is the availability of CPUs that support CXL at a production quality level. That will change with Granite Rapids and Turin being available. So at this point, what we can say is that we've been providing chips for quite some time. We've been in preproduction and supported the various different evaluation POC type of activities that have happened with our hyperscaler customers. So, to that standpoint, we do expect revenue to start coming in 2025 from memory expansion use case for CXL. That's the goal for the company. We have the COSMOS software and like I noted, PCI Express is one of those protocols which, unlike Ethernet, tends to be a little messy, meaning it's something that's been around for a long time. It's a great technology, but it also requires a lot of handholding. And for us, what has happened is being in the customers' platforms, bringing up systems that ramp up to millions of devices has allowed us to understand what are the nuances, what works, what doesn't work, how do you make the link perform at the highest rate. So that tribal knowledge is something that we've captured within the COSMOS software that we built running both on our chips as well as customers' platforms. So we do expect that as Gen 6 starts to materialize, lot of those learnings will be carried over. Now you're right that there's been a lot of competition that has come in as well. But we believe that when it comes to competition, they could have a similar product like us. But no matter what, there is a full time that's essential when it comes to connectivity type of chips, just given the interoperation and getting the kings out and so on. Meaning you could have a perfect chip yet have a failing system. The reason for that is the complexity of the system and how PCI Express standard is defined. So to that standpoint, I agree with what you said in the sense that we have the leading position now in the Retimer market for PCIe and we expect to build on that both with the new features we have added in PCIe Gen 6 or the AEC product line and also the tribal knowledge that we have built by working with our partners over the last three, four years. I don't know about the only, customer. I would probably request maybe you need to do some research on it on where the competition is. But from a Retimer standpoint, which goes on this, we do have a leading position. So based on that, I would imagine that we are the main provider here, both based on that and the customer traction that we're seeing. So, this one is an interesting use case. So far, PCI Express, as you know, was defined to be inside the server. But what is happening now, and this is why we're excited about PCIe, AECs, is that now we are opening up a new front in terms of clustering GPUs, meaning interconnecting accelerators. That is where the AECs will play, and that is a new opportunity that goes along with the Ethernet AECs that we already provide, which are also used for interconnecting GPUs on the backend network. So, overall, we do believe that combining our PCIe AEC solution and Ethernet AEC solution, we're well set for some of these evolving trends. And our revenue we expect to start coming in for the latter half of this year. And on PCIe, again, we do believe we are the only one just to make sure I clarify what I initially said, just that I don't know if there is someone else talking about it that's not yet in the public domain. Again, I think we, as a company don't talk about unreleased products, the timing of it. But what I can share with you is the following. First, we've been very fortunate to be in the central seat of AI deployment and enjoy a great relationship with the hyperscalers and AI platform providers. So we get to see a lot, we get to hear a lot in terms of some of the requirements. So clearly, we are going to be developing products that address the bottlenecks, whether it is on the data side, network side or on the memory side. So we are working on several products, as you can imagine, that would all be developed ground up for AI infrastructure and enable connectivity solutions that will deploy the AI application sooner. There is a lot going on, a lot of new infrastructure, a lot of new GPU announcements, CPU announcement. So, you can imagine given the pace of this market and the changes that are upcoming, we do anticipate that this will all start having meaningful impact and incremental revenue impact to our business. And just maybe if I can add to that, one way to visualize connectivity market or subsystem is the nervous system within the human anatomy. Right? It's one of those things where you don't want to mess with it. Yes. There will be a sick vendor. There are options or off-the-shelf. Once the nervous system is built, tested, especially like what we have developed where the nervous system that we've built is specifically done for AI application. And there's a lot of qualification, a lot of software investment that hyperscalers have done. And they want to reuse that across different kinds of topologies, whether it's ASIC based or merchant silicon based. And we do see a trend happening, when we look at customers that we're engaged with today. And for protocols like PCI Express, Ethernet, and CXL, and especially where as Taurus plays, these are standards based. So to that standpoint, whatever end possible architecture is being used, we believe that we will stand to gain from that. It's hard to say at this point just because there is so much of development going on here. I mean, you can imagine the non NVIDIA ecosystem, they will rely on standards technologies, whether it is PCI Express or Ethernet. And the advantage of PCI Express is that it's low latency. Right? Significantly low latency compared to Ethernet. So there are some benefits to that. And there are certain extensions that people consider to add on top of PCI Express, or when it comes to the proprietary implementation. So, overall, we do see this, from a technology standpoint, PCI Express will have that advantage. Now Ethernet also has been around, so we'll have to wait and see how all of this develops over the next, let's say, 6 to 18 months. Yes. So these are design wins to clarify. We have been shipping this. We announced this. We've demonstrated this at, at public forums. So to that standpoint, it's an opportunity that we're excited about and like we noted early on. We expect to start contributing revenue for later half of this year. Yes. There are two questions. Let me take the first one, which is the CXL side. For CXL, there are four main use cases to keep in mind, memory expansion, memory tiering, where you're trying to go for a TCO type of angle, memory pooling, and what is called as this memory drives that Samsung and others are providing. We believe memory drives are more suitable for the enterprise customers. And whereas the first three are more suitable for cloud scale deployment. And there, again, memory pulling is something that's further out in time is our belief just because it requires software changes. So the ones that are more sort of short-term, medium-term is memory expansion and memory tiering. And like I noted early on, all the major hyperscalers, at least in the U.S., are all engaged on the CXL technology. But it is going to be a matter of time with both CPUs being available and dollars being available from a general purpose compute standpoint. Okay. And then in terms of, your second question was, was that more on new products? Was that the context for it? Yes. So again, we don't talk about the exact time frame, but you can imagine our last product we announced was a little over a year ago. So our engineers have not been quite, so they've been working hard. So to that standpoint, we are working very diligently and hard based upon, lot of interest and engagement from customers that we've already been working with."
        }
    }
]